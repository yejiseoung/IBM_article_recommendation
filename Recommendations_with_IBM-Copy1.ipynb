{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/2322/view).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yejiseoung/Dropbox/My Mac (Yejis-MacBook-Pro.local)/Documents/Projects/IBM_article_recommendation'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/Users/yejiseoung/Dropbox/My Mac (Yejis-MacBook-Pro.local)/Documents/Projects/IBM_article_recommendation/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'user-item-interactions.csv')\n",
    "df_content = pd.read_csv(path/'articles_community.csv')\n",
    "\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1      1314.0       healthcare python streaming application demo   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45993, 3), (1056, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5148"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.email.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_interactions = df.groupby('email').count()['article_id']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(user_interactions, bins=10)\n",
    "plt.title('Distribution of how many articles a user interacts with')\n",
    "plt.xlabel('Interactions')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% of individuals interact with 3.0 number of articles or fewer.\n",
      "The maximum number of user-article interactions by any 1 user is 364.\n"
     ]
    }
   ],
   "source": [
    "# Fill in the median and maximum number of user_article interactions below\n",
    "\n",
    "median_val = user_interactions.median() # 50% of individuals interact with ____ number of articles or fewer.\n",
    "max_views_by_user = user_interactions.max() # The maximum number of user-article interactions by any 1 user is ______.\n",
    "print(\"50% of individuals interact with {} number of articles or fewer.\".format(median_val))\n",
    "print(\"The maximum number of user-article interactions by any 1 user is {}.\".format(max_views_by_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1056, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_body           1036\n",
       "doc_description    1022\n",
       "doc_full_name      1051\n",
       "doc_status            1\n",
       "article_id         1051\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "df_content.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "df_content.drop_duplicates(subset='article_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1051, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1      1314.0       healthcare python streaming application demo   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique atricles that have at least one interaction: 714\n",
      "The number of unique articles on the IBM platform: 1051\n",
      "The number of unique users: 5148\n",
      "The number of user-article interactions: 5148\n"
     ]
    }
   ],
   "source": [
    "unique_articles = df['article_id'].nunique() # The number of unique articles that have at least one interaction\n",
    "total_articles = df_content['article_id'].nunique() # The number of unique articles on the IBM platform\n",
    "unique_users = df['email'].nunique() # The number of unique users\n",
    "user_article_interactions = len(user_interactions) # The number of user-article interactions\n",
    "\n",
    "print(\"The number of unique atricles that have at least one interaction: {}\".format(unique_articles))\n",
    "print(\"The number of unique articles on the IBM platform: {}\".format(total_articles))\n",
    "print(\"The number of unique users: {}\".format(unique_users))\n",
    "print(\"The number of user-article interactions: {}\".format(user_article_interactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1429.0</th>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330.0</th>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431.0</th>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427.0</th>\n",
       "      <td>643</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364.0</th>\n",
       "      <td>627</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            title  email\n",
       "article_id              \n",
       "1429.0        937    937\n",
       "1330.0        927    927\n",
       "1431.0        671    671\n",
       "1427.0        643    643\n",
       "1364.0        627    627"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_viewed_articles = df.groupby('article_id').count().sort_values(by='email', ascending=False)\n",
    "most_viewed_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    937\n",
       "email    937\n",
       "Name: 1429.0, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_viewed_articles.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_article_id = '1429' # The most viewed article in the dataset as a string with one value following the decimal \n",
    "max_views = 937 # The most viewed article in the dataset was viewed how many times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...        1\n",
       "1      1314.0       healthcare python streaming application demo        2\n",
       "2      1429.0         use deep learning for image classification        3\n",
       "3      1338.0          ml optimization using cognitive assistant        4\n",
       "4      1276.0          deploy your python model as a restful api        5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## No need to change the code here - this will be helpful for later parts of the notebook\n",
    "# Run this cell to map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5956ddc973c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Test your dictionary against the solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msol_1_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msol_1_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "## If you stored all your results in the variable names above, \n",
    "## you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val,\n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions,\n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views,\n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id,\n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles,\n",
    "    '`The number of unique users in the dataset is ______`': unique_users,\n",
    "    '`The number of unique articles on the IBM platform`': total_articles\n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    top_articles = list(df.groupby('title').count().sort_values(by='user_id', ascending=False).head(n).index)\n",
    "    return top_articles # Return the top article titles from df (not df_content)\n",
    "\n",
    "\n",
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    top_articles = list(df.groupby('article_id').count().sort_values(by='user_id', ascending=False).head(n).index)\n",
    " \n",
    "    return top_articles # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use deep learning for image classification', 'insights from new york car accident reports', 'visualize car data with brunel', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'healthcare python streaming application demo', 'finding optimal locations of new store using decision optimization', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model']\n",
      "[1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0, 1162.0, 1304.0]\n"
     ]
    }
   ],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "#t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...        1\n",
       "1      1314.0       healthcare python streaming application demo        2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    # Fill in the function here\n",
    "    user_item = df.groupby(by=['user_id', 'article_id']).agg(lambda x: 1).unstack().fillna(0)\n",
    "    \n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5149, 714)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item.sum(axis=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have passed our quick tests!  Please proceed!\n"
     ]
    }
   ],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. \n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(user_item.loc[1,:], user_item.loc[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    # compute similarity of each user to the provided user\n",
    "    similarity = {}\n",
    "    \n",
    "    for user in user_item.index:\n",
    "        similarity[user] = np.dot(user_item.loc[user_id, :], user_item.loc[user, :])\n",
    "\n",
    "    # sort by similarity\n",
    "    sort_similarity = sorted(similarity.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    # create list of just the ids\n",
    "    most_similar_users = [key for (key, values) in sort_similarity]\n",
    "   \n",
    "    # remove the own user's id\n",
    "    most_similar_users.remove(user_id)\n",
    "       \n",
    "    return most_similar_users # return a list of the users in order from most to least similar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar users to user 1 are: [3933, 23, 3782, 203, 4459, 131, 3870, 46, 4201, 49]\n",
      "The 5 most similar users to user 3933 are: [1, 23, 3782, 203, 4459]\n",
      "The 3 most similar users to user 46 are: [4201, 23, 3782]\n"
     ]
    }
   ],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    article_names = [df[df['article_id']==float(id)]['title'].values[0] for id in article_ids]\n",
    "    \n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    article_ids = [str(id) for id in list(user_item.loc[user_id][user_item.loc[user_id]==1].title.index)]\n",
    "    article_names = get_article_names(article_ids)\n",
    "    \n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    recs = []\n",
    "    most_similar_users = find_similar_users(user_id)\n",
    "    the_user_articles, the_article_names = get_user_articles(user_id)\n",
    "    for user in most_similar_users:\n",
    "        article_ids, article_names = get_user_articles(user)\n",
    "        for id in article_ids:\n",
    "            if id not in the_user_articles:\n",
    "                recs.append(id)\n",
    "            if len(recs) >= m:\n",
    "                break\n",
    "        if len(recs) >= m:\n",
    "            break\n",
    "    \n",
    "    if len(recs) < m:\n",
    "        for id in str(df['article_id']):\n",
    "            if id not in the_user_articles:\n",
    "                recs.append(id)\n",
    "            if len(recs) >= m:\n",
    "                break\n",
    "    \n",
    "    return recs # return your recommendations for this user_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this week in data science (april 18, 2017)',\n",
       " 'timeseries data analysis of iot events by using jupyter notebook',\n",
       " 'got zip code data? prep it for analytics.  ibm watson data lab  medium',\n",
       " 'higher-order logistic regression for large datasets',\n",
       " 'using machine learning to predict parking difficulty',\n",
       " 'deep forest: towards an alternative to deep neural networks',\n",
       " 'experience iot with coursera',\n",
       " 'using brunel in ipython/jupyter notebooks',\n",
       " 'graph-based machine learning',\n",
       " 'the 3 kinds of context: machine learning and the art of the frame']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Results\n",
    "get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, you passed all of our tests!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Test your functions here - No need to change this code - just run this cell\n",
    "assert set(get_article_names(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names(['1320.0', '232.0', '844.0'])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set(['1320.0', '232.0', '844.0'])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    neighbors_df = pd.DataFrame(columns=['neighbor_id', 'similiary', 'num_interactions'])\n",
    "    for user in user_item.index:\n",
    "        if user == user_id:\n",
    "            continue\n",
    "        neighbors_df.loc[user] = [user, np.dot(user_item.loc[user_id, :], user_item.loc[user, :]),\n",
    "                                 df[df['user_id']==user]['article_id'].count()]\n",
    "    \n",
    "    neighbors_df.sort_values(by=['similiary', 'num_interactions'], ascending=False, inplace=True)\n",
    "    \n",
    "    return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    recs = []\n",
    "    \n",
    "    neighbors_df = get_top_sorted_users(user_id)\n",
    "    \n",
    "    the_user_articles, the_article_names = get_user_articles(user_id)\n",
    "    for user in neighbors_df['neighbor_id']:\n",
    "        article_ids, article_names = get_user_articles(user)\n",
    "        for id in article_ids:\n",
    "            if id not in the_user_articles:\n",
    "                recs.append(id)\n",
    "            if len(recs) >= m:\n",
    "                break\n",
    "        if len(recs) >= m:\n",
    "            break\n",
    "            \n",
    "    if len(recs) < m:\n",
    "        for id in [str(id) for id in get_top_article_ids(100)]:\n",
    "            if id not in the_user_articles:\n",
    "                recs.append(id)\n",
    "            if len(recs) >= m:\n",
    "                break\n",
    "                \n",
    "    rec_names = get_article_names(recs)\n",
    "    \n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "['12.0', '109.0', '125.0', '142.0', '164.0', '205.0', '302.0', '336.0', '362.0', '465.0']\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['timeseries data analysis of iot events by using jupyter notebook', 'tensorflow quick tips', 'statistics for hackers', 'neural networks for beginners: popular types and applications', 'learn tensorflow and deep learning together and now!', \"a beginner's guide to variational methods\", 'accelerate your workflow with dsx', 'challenges in deep learning', 'dsx: hybrid mode', 'introduction to neural networks, advantages and applications']\n"
     ]
    }
   ],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = find_similar_users(1)[0] # Find the user that is most similar to user 1 \n",
    "user131_10th_sim = find_similar_users(131)[9] # Find the 10th most similar user to user 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user that is most similar to user 1: 3933\n",
      "The 10th most similar user to user 131: 242\n"
     ]
    }
   ],
   "source": [
    "print(\"The user that is most similar to user 1: {}\".format(user1_most_sim))\n",
    "print(\"The 10th most similar user to user 131: {}\".format(user131_10th_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "#t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we do not have any information about new users, `get_top_article_ids` would be useful when we recommend users who have not viewed any articles yet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below.  You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "new_user_recs = [str(id) for id in get_top_article_ids(10)] # Your recommendations here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "assert set(new_user_recs) == set(['1314.0','1429.0','1293.0','1427.0','1162.0','1364.0','1304.0','1170.0','1431.0','1330.0']), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations (EXTRA - NOT REQUIRED)</a>\n",
    "\n",
    "Another method we might use to make recommendations is to perform a ranking of the highest ranked articles associated with some term.  You might consider content to be the **doc_body**, **doc_description**, or **doc_full_name**.  There isn't one way to create a content based recommendation, especially considering that each of these columns hold content related information.  \n",
    "\n",
    "`1.` Use the function body below to create a content based recommender.  Since there isn't one right answer for this recommendation tactic, no test functions are provided.  Feel free to change the function inputs if you decide you want to try a method that requires more input values.  The input values are currently set with one idea in mind that you may use to make content based recommendations.  One additional idea is that you might want to choose the most popular recommendations that meet your 'content criteria', but again, there is a lot of flexibility in how you might make these recommendations.\n",
    "\n",
    "### This part is NOT REQUIRED to pass this project.  However, you may choose to take this on as an extra way to show off your skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yejiseoung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yejiseoung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yejiseoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yejiseoung/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords', 'averaged_perceptron_tagger'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Function to tokenize an article title\n",
    "    \n",
    "    INPUT\n",
    "        text (str): artile titles\n",
    "    OUTPUT\n",
    "        tokens (list): a list of words\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize and remove puctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens\n",
    "             if word not in stop_words]\n",
    "    \n",
    "    # remove short words\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_body           0.013321\n",
       "doc_description    0.002854\n",
       "doc_full_name      0.000000\n",
       "doc_status         0.000000\n",
       "article_id         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.isnull().sum()/len(df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Heres this weeks news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2   * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "2  Heres this weeks news in Data Science and Bi...   \n",
       "3  Learn how distributed DBs solve the problem of...   \n",
       "4  This video demonstrates the power of IBM DataS...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  \n",
       "2         This Week in Data Science (April 18, 2017)       Live           2  \n",
       "3  DataLayer Conference: Boost the performance of...       Live           3  \n",
       "4      Analyze NY Restaurant data using Spark in DSX       Live           4  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1051, 5)\n",
      "(1034, 5)\n"
     ]
    }
   ],
   "source": [
    "df_content_copy = df_content.copy()\n",
    "print(df_content.shape)\n",
    "df_content_copy = df_content_copy.dropna()\n",
    "print(df_content_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content_copy[df_content_copy['doc_description'].isnull()==True] = 'Not information'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_body, doc_description, doc_full_name, doc_status, article_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_copy[df_content_copy['doc_description']=='Not information']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_body, doc_description, doc_full_name, doc_status, article_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_copy[df_content_copy['doc_description'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>The search index lets you create flexible quer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Build the search index in Cloudant</td>\n",
       "      <td>Live</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Compose The Compose logo Articles Sign in Free...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Announcing the Data Browser for JanusGraph</td>\n",
       "      <td>Live</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>Cloudant Query is a powerful declarative JSON ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Use the new Cloudant query</td>\n",
       "      <td>Live</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_body doc_description  \\\n",
       "354  The search index lets you create flexible quer...             NaN   \n",
       "768  Compose The Compose logo Articles Sign in Free...             NaN   \n",
       "919  Cloudant Query is a powerful declarative JSON ...             NaN   \n",
       "\n",
       "                                  doc_full_name doc_status  article_id  \n",
       "354          Build the search index in Cloudant       Live         354  \n",
       "768  Announcing the Data Browser for JanusGraph       Live         765  \n",
       "919                  Use the new Cloudant query       Live         916  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content[df_content['doc_description'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect bad readings in real time using Python and Streaming Analytics.\n",
      "----\n",
      "See the forest, see the trees. Here lies the challenge in both performing and presenting an analysis. As data scientists, analysts, and machine learning engineers faced with fulfilling business obj\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "Learn how distributed DBs solve the problem of scaling persistent storage, but introduce latency as data size increases and become I/O bound.\n",
      "----\n",
      "This video demonstrates the power of IBM DataScience Experience using a simple New York State Restaurant Inspections data scenario. \n",
      "----\n",
      "Using Compose's PostgreSQL data browser.\n",
      "----\n",
      "Upgrading your PostgreSQL deployment to version 9.5 is now possible through the Compose console. Working out how to perform this upgrade safely and reliably has been an interesting process because from version 9.4 to 9.5 is a PostgreSQL major upgrade.\n",
      "----\n",
      "For a company like Slack that strives to be as data-driven as possible, understanding how our users use our product is essential. The Data Engineering team at Slack works to provide an ecosystem to\n",
      "----\n",
      "Kaggle is your home for data science. Learn new skills, build your career, collaborate with other data scientists, and compete in world-class machine learning challenges.\n",
      "----\n",
      "[A version of this post appears on the OReilly Radar.] The OReilly Data Show podcast: Fang Yu on data science in security, unsupervised learning, and Apache Spark. Subscribe to the OReilly\n",
      "----\n",
      "Apple's sample app, Food Tracker, taught you iOS. Now, take it further and sync data between devices, through the cloud, with an offline-first design.\n",
      "----\n",
      "Replicating data to a relational dashDB database greatly enhances your options to analyze that data. In addition to the ability to query the warehouse with SQL, you can use the power of the statistical R language to do the analysis. Now, we have extended Cloudants warehousing capability to include GeoJSON documents. An ever increasing number of mobile and internet-of-things (IOT) applications capture and store geospatial data in NoSQL databases such as those provided by Cloudant. GeoJSON is the de-facto standard for such data. This new capability enables your data analysis to reflect geospatial aspects.\n",
      "----\n",
      "This recipe showcases how one can analyze the historical time series data, captured on the IBM Watson IoT platform, in a Jupyter Notebook using Spark SQL and Pandas DataFrames. Also, use the pre-installed matplotlib library to visualize results. \n",
      "----\n",
      "Theres a reason youve been hearing a lot about data science notebooks lately: data scientists are in high demand, and the Python programming language is widely used. In particular, Jupyter\n",
      "----\n",
      "Who are those people lurking behind the statistics in your data? Whether you are looking at retail shoppers, insurance policy holders, banking customers or political constituents, the more you can\n",
      "----\n",
      "Early methods to integrate machine learning using Naive Bayes and custom sinks.\n",
      "----\n",
      "The performance of supervised predictive models is characterized by the generalization error, that is, the error, obtained on datasets different from the one used to train the model.\n",
      "----\n",
      "We've always considered MySQL as a potential Compose database, but had to wait for the arrival of a high availability solution that worked well enough that we could deliver it to Compose users. That solution came in the form of MySQL InnoDB Cluster.\n",
      "----\n",
      "It has never been easier to build AI or machine learning-based systems than it is today. The ubiquity of cutting edge open-source tools such as TensorFlow, Torch, and Spark, coupled with the\n",
      "----\n",
      "In our Metrics Maven series, Compose's data scientist shares database features, tips, tricks, and code you can use to get the metrics you need from your data. In this article, we'll have a look at mode to round out our series on mean, median, and mode.\n",
      "----\n",
      "It is often useful to use RStudio for one piece of your analysis and notebooks (whether in R, or in another language) for other parts of your analysis. This article will step you through the process\n",
      "----\n",
      "Youre doing your data a disservice if you dont use maps. Nearly all data has a spatial component  customer locations, crime, election results, traffic incidents, points-of-purchase, infrastructure\n",
      "----\n",
      "Introducing nosqlimport, an npm module to help you import comma-separated and tab-separated files into your JSON document store of choice.\n",
      "----\n",
      "This video shows you how to build and query a Cloudant Geospatial index using the new Maps in the Cloudant dashboard!\n",
      "----\n",
      "Botkit provides a simple framework to handle the basics of creating a chat application. What follows is an example which is not even a toy app but we will carry it no further. \n",
      "----\n",
      "Want to learn more about how we created the Data Science Experience? We've interviewed hundreds of data scientists and analyzed how they think, how they lear...\n",
      "----\n",
      "Much of driving is spent either stuck in traffic or looking for parking. With products like Google Maps and Waze, it is our long-standing goal to help people navigate the roads easily and efficiently. But until now, there wasnt a tool to address the all-too-common parking woes.\n",
      "----\n",
      "This talk assumes you have a basic understanding of Spark and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs  this is the talk for you.\n",
      "----\n",
      "In this paper, we propose gcForest, a decision tree ensemble approach with performance highly com- petitive to deep neural networks. \n",
      "----\n",
      "Im very happy and proud to announce that IBM is the first non-academic supplier to offer a data science course on Coursera. Weve worked very hard to make this course a great learning experience for\n",
      "----\n",
      "An open API is available on the internet for free. We review the growth of API economy and how organizations have been realizing the potential of open APIs in transforming their business. \n",
      "----\n",
      "This video shows you how to sign up for a free trial of IBM Data Science Experience (DSX).\n",
      "----\n",
      "Stacking is a model ensembling technique used to combine information from multiple predictive models to generate a new model. Often times the stacked model will outperform each of the individual mo\n",
      "----\n",
      "Analytics and visualization often go hand-in-hand.  One of the great things about notebooks such as IPython/Jupyter is that they provide a single interface to numerous data analysis technologies that often can be used together.  So, using Brunel within notebooks is a very natural fit.  For example, I can use a wide variety of python libraries to cleanse, shape and analyze dataand then use Brunel to visualize those results.\n",
      "----\n",
      "Machine learning has already extended into so many aspects of daily life that it can be handy for us to simply memorize a set of go-to examples of its impact on certain industries. For instance, we\n",
      "----\n",
      "In this article, Ill describe an app I built to help with my Reddit game, and what I learned about machine learning in the process. Ill also share the code so you can try it yourself.\n",
      "----\n",
      "Heres a quick and handy guide to creating data visualizations that are appropriately detailed to ensure maximum effectiveness.\n",
      "----\n",
      "When you customise your Cloudant domain with Cloudfare, you get better performance, DDoS protection, and caching too. Heres how to set it up.\n",
      "----\n",
      "A guide to using Cloudant's _all_docs endpoint to retrieve documents by id, or within a range of keys using this interactive tutorial.\n",
      "----\n",
      "Our app will be simple in that it displays price histories, but it can serve as the foundation for more complicated work, as we will discuss when the app is completed in the next post. At the outset, it is crucial to note that this Notebook will serve a different purpose than our previous Notebook.\n",
      "----\n",
      "Ensemble learning helps improve machine learning results by combining several models. Ensemble methods allow the production of better predictive performance compared to a single model. \n",
      "----\n",
      "It's easy to customize the Mongo shell's prompt, especially if you use MongoDB shell with one of MongoHQ's Elastic Deployments.\n",
      "----\n",
      "Getting started with ScyllaDB is easy since it is a drop in replacement for Apache's Cassandra database.\n",
      "----\n",
      "This free Deep Learning with TensorFlow course provides a solid introduction to the use of TensorFlow to analyze unstructured data.\n",
      "----\n",
      "Learn how to use IBM Bluemix and the Simple Data Pipe example app to conduct Stack Overflow analysis on how well users support certain tech products.\n",
      "----\n",
      "Build a custom library for Apache Spark and deploy it to a Jupyter Notebook.\n",
      "----\n",
      "RethinkDB's push updates makes it great for real-time apps. Here's an example built for live Q&A sessions at conferences using RethinkDB changefeeds.\n",
      "----\n",
      "Watch how to download and install Database Conversion Workbench, Data Studio plugin for IBM dashDB.\n",
      "----\n",
      "Learn to use IBM Data Science Experience.\n",
      "----\n",
      "We'll also look at using PostGIS to filter our data and to find places that are within or intersect a chosen polygon.\n",
      "----\n",
      "Community Detection at Scale\n",
      "----\n",
      "Get to know the ML landscape through this practical, concise overview of modern machine learning algorithms. Plus, we'll discuss the tradeoffs of each.\n",
      "----\n",
      "Watch how to build a storefront web app with IBM Graph.\n",
      "----\n",
      "Starting today, users will be able to access Streams Designer through the Watson Data Platform. Streams Designer is a brand new IDE for building applications using real time data. \n",
      "----\n",
      "Discover eight ways that Apache Sparks machine learning capabilities are driving the modern business.\n",
      "----\n",
      "Build a Machine Learning model with Apache Spark MLLib to predict flight delays based on weather data and past performance.\n",
      "----\n",
      "Easily add autocomplete to your web form fields. Simply upload your data set using this cloud service then use its fast and efficient autocomplete API.\n",
      "----\n",
      "In this work, we explore improving a vanilla regression model with knowledge learned elsewhere. \n",
      "----\n",
      "01. Holden Karau, IBM, Visits #theCUBE!. (00:21) 02. Give Us An Update On Spark. (00:43) 03. Do The Hardcore Spark Developers Have To Main Stream It. (01:48)...\n",
      "----\n",
      "Lets say you have the gift of flight (or you are riding a chopper). You are also a Spy (like in James Bond movies). You are given the\n",
      "----\n",
      "readr 1.0.0 is now available on CRAN. readr makes it easy to read many types of rectangular data, including csv, tsv and fixed width files. Compared to base equivalents like read.csv(), readr is mu\n",
      "----\n",
      "If you use PostgreSQL, you're probably already familiar with many of the common aggregate functions, such as COUNT(), SUM(), MIN(), MAX(), and AVG(). But you may not be familiar with window functions since they're touted as an advanced feature. Window functions aren't nearly as esoteric as they may seem, however. Let's dive in!\n",
      "----\n",
      "Find out how including too much information can neutralize your data visualizationand your message with it.\n",
      "----\n",
      "PostgreSQL is a powerhouse of SQL-driven database power and Compose's PostgreSQL is all that with the power of Compose's cloud deployments. But before you can harness that power you need to create users to access your database.\n",
      "----\n",
      "Open data and open-source analytics allows community stakeholders to mine data for actionable intelligence like never before.  The objective of this research is to take a first step in exploring the feasibility of forecasting neighborhood change using longitudinal census data in 29 Legacy Cities.\n",
      "----\n",
      "If youre an IBM customer or business partner, youve probably heard of the companys InterConnect conference. If youre unfamiliar, IBM InterConnect is a huge conference at the Mandalay Bay in Las\n",
      "----\n",
      "I'm happy to introduce to you a new sample app called Cloudant FoodTracker which demonstrates building an offline-first app using Cloudant Sync for iOS.\n",
      "----\n",
      "My new job as a Developer Advocate with IBM means I get to play with databases for a living (this is the most awesome thing ever invented, seriously). On my travels, I spent some time with MongoDB \n",
      "----\n",
      "An introductory overview of NumPy, one of the foundational aspects of Scientific Computing in Python, along with some explanation of the maths involved.\n",
      "----\n",
      "Figuring out how to calculate a moving average can be a bit daunting if you've never done it. Once you learn a method you like, though, (we'll cover two) it's easy to do and you'll find many uses for it in your tracking and reports.\n",
      "----\n",
      "Bradley Holt, IBM Cloudant Web and mobile apps shouldn't stop working when there's no network connection. Based on Apache CouchDB, PouchDB is an open source ...\n",
      "----\n",
      "Tutorial for creating a web-tracking app that works with dynamically-generated UI elements. Uses Node.js, Cloudant, and IBM Bluemix.\n",
      "----\n",
      "Just over a year ago we asked Is PostgreSQL Your Next JSON Database... Now, with PostgreSQL 9.5 out, it's time to check if Betteridge's law still applies. So let's talk about JSONB support in PostgreSQL 9.5.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "What do you do for a living? That question used to have a pretty clear answer: Im a data scientist. But lately, its gotten more complicated\n",
      "----\n",
      "This video provides a tour of the Community section in IBM Data Science Experience. \n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "While the sum of Facebook's offerings covers a broad spectrum of the analytics space, we continually interact with the open source community in order to share our experiences and also learn from others.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "See how a Cloudant Geospatial index is used in a real world application. \n",
      "----\n",
      "This post discusses how to implement Apples new Core ML platform within DSX, which was announced a few days ago at WWDC 2017. Core ML is a platform that allows integration of powerful pre-trained\n",
      "----\n",
      "Using cooperative learning approaches to generate entity vectors.\n",
      "----\n",
      "This video shows how to create, train, save, and deploy a logistic regression model using IBM Waston Machine Learning and IBM Data Science Experience that assesses the likelihood that a customer of an outdoor equipment company will buy a tent based on age, sex, marital status and job profession.\n",
      "----\n",
      "Today, we are proud to announce that JanusGraph is coming to Compose and will bring with it the power of fully open source graph databases.\n",
      "----\n",
      "Watch how easy it is to consume Twitter data with IBM dashDB for further analytics.\n",
      "----\n",
      "testthat 1.0.0 is now available on CRAN. Testthat makes it easy to turn your existing informal tests into formal automated tests that you can rerun quickly and easily. Learn more at Install the lat\n",
      "----\n",
      "Mean, median and mode - these three metrics can provide valuable insights from your data. Over the next few articles we'll get deeply familiar with each of them and also highlight some key considerations for understanding what's really contributing to the numbers we report.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "Our Metrics Collector app rebuilt to fit into a Microservices stack.\n",
      "----\n",
      "The CRAN Package repository features 6778 active packages. Which of these should you know? Here is an analysis.  \n",
      "----\n",
      "We're pleased to announce that we are now enabling importing on MongoDB on Compose.\n",
      "----\n",
      "Website Engagement Tracking is a technique that allows businesses to see which parts of their website users are visiting, clicking on, and viewing. In this article, we'll take a look at tracking user engagement using Elasticsearch on Compose.\n",
      "----\n",
      "If you wish to begin a career in data science, you can save yourself days, weeks, or even months of frustration by avoiding these 9 costly beginner mistakes.\n",
      "----\n",
      "Read how to take a standard PHP application and deploy to Bluemix.\n",
      "----\n",
      "Armand Ruiz Gabernet (@armand_ruiz), Lead Product Manager - IBM Data Science Experience, IBM, sits down with Dave Vellante & Jeff Frick on the #theCUBE at #B...\n",
      "----\n",
      "Our forty fifth release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "Discover an open source machine learning platform that combines the data processing power of Spark with powerful machine learning algorithms.\n",
      "----\n",
      "Our thirty ninth release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "Sign up now to learn about data visualization in R\n",
      "----\n",
      "Looking to learn the basics of cloud databases? In this series, we show them running on Compose and intro programmatic access. Enter: MongoDB + PHP.\n",
      "----\n",
      "This video shows you how to add a data asset to an existing project and then load that data for analytics in a Python notebook. \n",
      "----\n",
      "Projects can be great for mastering data science, but you have to choose your projects carefully. This article will give you tips on how to choose a project that's appropriate for your skill level (and tell you some pitfalls to watch out for). For more data science tutorials, sign up for our email list.\n",
      "----\n",
      "Ever had to make a decision when you didnt have the time, means or patience to look up all the data that could help you choose the best option? Yes, well, youre not alone on that score. \n",
      "----\n",
      "by Sean LoppAt RStudio, we work with many companies interested in scaling R. They typically want to know:How can R scale for big data or big computation?How can R scale for a growing team of data scientists?This post provides a framework for answering both questions.Scaling R for Big Data or Big ComputationThe first step to\n",
      "----\n",
      "Our fortieth release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "A summary of Progressive Web Apps and recommendations on refactoring code to use offline-first storage and other aspects of PWAs.\n",
      "----\n",
      "Find out why redundant visualizations can turn detail into too much of a good thing, obscuring connections and diminishing contrast.\n",
      "----\n",
      "Machine learning is often the enhancer of a product.\n",
      "----\n",
      "Create a notebook using IBM Data Science Experience using PixieDust to explore and visualize data in different ways (e.g., charts, maps, etc.) with one simple call.\n",
      "----\n",
      "A weekly newsletter about the latest developments in Deep Learning.\n",
      "----\n",
      "An open source helper library for your Jupyter Python notebook with easier data viz & export, package manager, and Scala context from within Python!\n",
      "----\n",
      "Learn how to use scripts and external packages in Jupyter notebooks to facilitate code organization for larger projects.\n",
      "----\n",
      "What is Apache SystemML? Demo! How to get SystemML.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "Loren Sands-Ramshaw, author of GraphQL: The New REST, shows how to combine data from multiple sources using GraphQL in this Write Stuff two-part series.\n",
      "----\n",
      "When the IBM Design team began researching data scientists, we had a lot to learn; but what we found was our two disciplines had a lot in common.\n",
      "----\n",
      "In this tutorial, the complete concept of random forest and bagging is explained.\n",
      "----\n",
      "This post provides a brief summary of sample code changes to migrate a Java application from Apache SparkTM 1.6 to Apache SparkTM 2.0. The migration effort is dependent upon the Apache SparkTM APIs a given application uses.\n",
      "----\n",
      "One Preschool has been a loyal Compose customer since their inception. They have been on a mission to make preschooling both accessible and practical throughout the world. We spoke with co-founders Chris Bennett and Arrel Gray to hear about their startup, technology stack, and mission.\n",
      "----\n",
      "How to run Spark batch jobs programmatically. See examples in both Scala and Python that launch a Hello World Spark job via spark-submit.\n",
      "----\n",
      "The Machine Learning revolution is underway and is changing industries and delivering outcomes that were unimaginable a few years ago. In this video of John J. Thomass keynote at ApacheCon on May 17\n",
      "----\n",
      "Here's a question we hear a lot - Should I use SQL databases or NoSQL databases? It's a question that gets asked because often underlying it is another question - What's broken in SQL databases that NoSQL databases fixes? The answer to that one is much easier: Nothing.\n",
      "----\n",
      "This is the first installment in a series of posts aimed at introducing developers like me and you to the basic machine learning concepts and tools required to get an ML system up and running.\n",
      "----\n",
      "When it's time to choose the database technology for your app, the choices can be overwhelming. Here's why you should consider graph databases.\n",
      "----\n",
      "Machine learning studies the design of algorithms that can learn. The hope that this discipline brings with itself is that the inclusion of experience into its tasks will eventually improve the\n",
      "----\n",
      "Musings on data science and software engineering (and at times, economics as well)\n",
      "----\n",
      "Learn about JOINs in the RethinkDB document database.\n",
      "----\n",
      "Meteor database driver for CouchDB and Cloudant\n",
      "----\n",
      "Cloudant Search is based on Apache Lucene which allows facets of your data to be aggregated and counted during the search process. Facets allow your customers to drill-down into the search results, filtering in an powerful and intuitive way.\n",
      "----\n",
      "Sasko Stubailo of Meteor explains what GraphQL is, what data management problems it can solve in an organization, and how you can try it today.\n",
      "----\n",
      "Feature importance in machine learning using examples in Python with xgboost. Getting better performance from a model with feature pruning.\n",
      "----\n",
      "This article is a follow on to the previous article on analyzing data with python, building on the basic intro of IPython, notebooks and pandas to show how to visualize the data you have processed with these tools.\n",
      "----\n",
      "Python and R cheat sheets specifically for those who are just starting out with data science or for those who need an extra help when working on data science problems.\n",
      "----\n",
      "With the release of Elasticsearch 5.x came Painless, Elasticsearch's answer to safe, secure, and performant scripting. We'll introduce you to Painless and show you what it can do.\n",
      "----\n",
      "Our thirty eighth release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "Following CouchDB's latest release, how do you back up a CouchDB or Cloudant database?\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "This video shows you how to execute some common HTTP API commands to create, read, update, and delete data in a Cloudant database. \n",
      "----\n",
      "dplyr and data.table are amazing packages that make data manipulation in R fun. Both packages have their strengths. While dplyr is more elegant and resembles natural language, data.table is succinct\n",
      "----\n",
      "Gigi Sayfan takes us through doing just that with Cassandra/Scylla, MySQL, and Redis. In this Compose's Write Stuff article, he shows us how he constructs his own Moneyball.\n",
      "----\n",
      "Another video from our DataLayer Conference, featuring Emile Baizel from Digit.\n",
      "----\n",
      "More analysis options for Simple Data Pipe output. Once data is in Cloudant, connect via CouchDB then analyze the JSON with SparkR in a Jupyter notebook.\n",
      "----\n",
      "An introduction to neural networks for beginners: the main challenges of working on neural networks, their popular types and applications.\n",
      "----\n",
      "To understand Scala even better, I sat down with Jakob Odersky, a real-life, bonafide Scala expert, to ask him a few seriously Scala questions.\n",
      "----\n",
      "Our twenty second release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "Just recently in the UK, weve seen the dangers of making decisions based on incomplete or poor data play out on the world stage. The Prime Minister called a general election three years earlier than\n",
      "----\n",
      "We have heard from many of you that Db2 Warehouse on Cloud is the relational database of choice for use in DSX. Today, Im happy to announce a new feature that makes it even easier to use your Db2\n",
      "----\n",
      "Build a data collection app that captures and stores QR Code data, even when your network is unavailable.\n",
      "----\n",
      "Search your Slack account using an IBM Graph database and Watson's AlchemyAPI.\n",
      "----\n",
      "See how easy it is to unlock your data for use in mobile and web applications, or for more flexible analysis and reporting. Bluemix Secure Gateway service lets you move data from on-premises to the cloud in a secure manner. \n",
      "----\n",
      "Campus Discounts uses several Compose-hosted databases including MySQL, MongoDB, Redis, Elasticsearch and RabbitMQ to power their social media platform. Recently they started exploring IBM Watson to add cognitive features to the app. We sat down with founder and CTO Don Omondi to hear their story.\n",
      "----\n",
      "Data science is about learning by doing. One of the ways you can learn how to do data science is by building your own portfolio: elaborating your own pet project, doing a quick data exploration task\n",
      "----\n",
      "After leading hundreds of projects a year and gaining advice from top teams all over the United States, we wrote this post to explain how to build Machine Learning solutions to solve problems like the ones mentioned above.\n",
      "----\n",
      "In Part 1 of this series, we wrote about our goal to explore a use case and use various machine learning platforms to see how we might build classification models with those platforms to predict\n",
      "----\n",
      "Watch how quick and easy it is to perform analytics with dashDB and Tableau. \n",
      "----\n",
      "In this article, we'll walk through how and why to calculate an exponentially weighted moving average.\n",
      "----\n",
      "Jonas Helfer from Meteor discussing joins across databases with GraphQL.\n",
      "----\n",
      "There are as many approaches to selecting features as there are statisticians since every statistician and their sibling has a POV or a paper on the subject. This is an overview of some of these approaches.\n",
      "----\n",
      "Were pleased to announce d3heatmap, our new package for generatinginteractive heat maps usingd3.jsand htmlwidgets. Tal Galili, author of dendextend, collaborated with us on this package. \n",
      "----\n",
      "In preparation for the SETI Institutes Hackathon and Code Challenge, a citizen scientist, Dr. Arun Ramamoorthy, who is also a researcher from Arizona State University, was looking at data from the\n",
      "----\n",
      "Our Location Tracker example app shows how simple it is to use Cloudant with Swift + GeoJSON. It's offline-first and scales up the database per user pattern.\n",
      "----\n",
      "How to use the Spark machine learning programming model in IBM Analytics for Apache Spark on IBM Bluemix\n",
      "----\n",
      "This post is the first in a two-part series on stock data analysis using R. In these posts, basics such as obtaining the data from Yahoo! Finance using pandas, visualizing stock data, moving averages, developing a moving-average crossover strategy, backtesting, and benchmarking will be covered.\n",
      "----\n",
      "There's now the option to power up more Compose Portals and we've made a recently introduced feature, Deleted Deployments, easier to work with - in this Compose Notes, we'll tell you all about them.\n",
      "----\n",
      "In this article, we discuss various Deep Learning approaches and recommend you a way to learn TensorFlow and Deep Learning at the same time.\n",
      "----\n",
      "Classcraft gamifies the whole classroom experience, making education a fun adventure for both students and teachers. We chatted with Shawn Young, ex-teacher, programmer, and founder of Classcraft Studios about their teaching platform built on Meteor.js and their use of Compose for MongoDB and Elasticsearch.\n",
      "----\n",
      "Don Omondi demonstrates how to leverage Compose MongoDB to send more effective push notifications.\n",
      "----\n",
      "Cloudant Local brings Cloudant's NoSQL database to customers whose regulatory restrictions make it difficult to put their data into the cloud.\n",
      "----\n",
      "Python and Node.js in the same Jupyter notebook (part 1).\n",
      "----\n",
      "TL;DR: RethinkDB databases on Compose will be available for the foreseeable future.\n",
      "----\n",
      "Douglas Mason [Data Scientist at Twitter]  What I carried with mewas a relentless enthusiasm. Im certain that was key to my success, leading to my current role as a Data Scientist a\n",
      "----\n",
      "Earlier this year, RethinkDB released their Fantasia 2.3 version. In this article, we're going to take a closer look at one of the lesser-known features that came out with that release - the aggregation fold command.\n",
      "----\n",
      "Import from the cloud data source of your choice. How connectors let you load data from a variety of sources, through the Simple Data Pipe, and into Cloudant.\n",
      "----\n",
      "Technology is advancing and new methods of designing effective business-progressing tools are emerging. Data Science conferences are not only about discovering the latest trends in the field, but\n",
      "----\n",
      "Quickly create a faceted search API for use in your own apps with open source code for Cloudant & Redis, from IBM's CDS developer advocacy team.\n",
      "----\n",
      "Some people have a way with words. Others have a way with code. If youre using my Poem Generator project, you dont necessarily need either. The application creates poems based on user input. Using\n",
      "----\n",
      "Join us for a look at whats on the horizon in data analytics, discovering how a broad array of tools aims to change the way we doand think aboutdata science.\n",
      "----\n",
      "OpenWhisk makes it easy to deploy microservices and eliminates the need to manage your own message broker or deploy your own worker servers.\n",
      "----\n",
      "With the most recent version of PostgreSQL gaining ever more JSON capabilities, we've been asked if PostgreSQL could replace MongoDB as a JSON database.\n",
      "----\n",
      "In this article, we'll use Node-RED and MongoDB to build a minimal RESTful API for a photographer's portfolio website. \n",
      "----\n",
      "Offline Camp is a gathering of the Offline First community, coming together to hack on offline first problems over a long weekend away from it all.\n",
      "----\n",
      "If you are a Science or Math nerd, there is no way in hell you would have not heard of Bayess Theorem. Its pervasive and quite a powerful\n",
      "----\n",
      "This video is a quick tour of the RStudio Integrated Development Environment inside IBM Data Science Experience (DSX). \n",
      "----\n",
      "James Kobielus, data science evangelist at IBM, interviews Holden Karau, principal software engineer of big data at IBM and coauthor of Learning Spark.\n",
      "----\n",
      " We believe the best way to boost the performance and ROI of an analytical model is by investing in new sources of data which can help to further unravel complex customer behavior and improve key analytical insights.\n",
      "----\n",
      "With Cloudant, building location-aware systems is within the reach of any web developer. This demo application uses HTML5 and JavaScript to record a device's GPS locations, and then save themboth on the device and to IBM Cloudant.\n",
      "----\n",
      "A powerful way to fine-tune location search results. Combine basic geospatial queries with Lucene's extraordinary text search capabilities.\n",
      "----\n",
      "TL;DR: The latest RethinkDB drivers don't work with previous versions of RethinkDB. Take steps to \"pin\" your drivers to a compatible version.\n",
      "----\n",
      "In the domain of data science, solving problems and answering  questions through data analysis is standard practice. Often,  data scientists construct a model to predict outcomes or  discover underlying patterns, with the goal of gaining insights.  Organizations can then use these insights to take actions that  ideally improve future outcomes.\n",
      "----\n",
      "Common excel tasks in pandas part\n",
      "----\n",
      "Today, Compose is bringing horizontal scaling to more databases on our Enterprise platform.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "My IoT talk in Hull included an Offline-First approach. Store data on a local device and sync to the cloud later using Apache CouchDB and IBM Cloudant.\n",
      "----\n",
      "Today we are announcing support for Apache Spark 2.1 and enhanced Spark job monitoring in the IBM Data Science Experience. The latest official release of Spark comes with plenty of new features\n",
      "----\n",
      "Listen to this podcast where Roger Rea, Senior Offering Manager for IBM Streams, shares his thoughts on how data scientists can create real-time applications using IBM Streams.\n",
      "----\n",
      "My view is that AI systems are increasingly necessary to augment what we do in our everyday liveswhether that means So, why all the fear?\n",
      "----\n",
      "Compose provides us with deployments that include security defaults which can be further expanded to reduce risk. In this post, I hope to explain some basic security practices to lock down access to a MongoDB deployment from VPC.\n",
      "----\n",
      "Cloudant Query is the best way to get started with querying Cloudant databases; a simple API call is used to define the list of fields to be indexed. Under the hood, Cloudant Query can leverage various indexes to provide a full breadth of querying capabilities.\n",
      "----\n",
      "Introducing a refactored Pipe architecture for cloud data movement. Connect to REST data sources, and land data all in one place, in its native structure.\n",
      "----\n",
      "Daniel Doubrovkine, CEO of Artsy.net and 2016 Ruby prize award nominee, took the stage. Daniel presented Artsy.net's Art Genome Project, a classification system and technological framework that powers Artsy.\n",
      "----\n",
      "From autocorrect to Google Maps, AI has already started picking up some of the slack. AI revolutionizes industries and our daily lives.\n",
      "----\n",
      "This week we're introducing our first talk from DataLayer Conf, the Keynote with Mitch Pirtle from CaptialOne.\n",
      "----\n",
      "Open data is freely available, which means you can modify, store, and use it without any restrictions. Governments, academic institutions, and publicly focused agencies are the most common providers\n",
      "----\n",
      "Autocomplete is everywhere. As you type into a web form, the page offers you completions that match. An autocomplete service consists of three components: When the data size is small (say, less than\n",
      "----\n",
      "We're bringing you video of all the sessions from this year's DataLayer conference, starting with the opening keynote from Charity Majors on Observability.\n",
      "----\n",
      "Variational Bayeisan (VB) Methods are a family of techniques that are very popular in statistical Machine Learning. VB methods allow us to r...\n",
      "----\n",
      "Watch how to convert XML data to CSV format to load into dashDB. This video shows a tool called Convert XML to CSV found here: http://www.convertcsv.com/xml-to-csv.htm\n",
      "----\n",
      "Working with dates in MongoDB can be surprisingly nuanced. Here, we examine the inner workings of MongoDB dates and show how to choose the right date type for your needs.\n",
      "----\n",
      "Users can now use either the original CouchDB view-based indexes or the new search-based indexes to query Cloudant and CouchDB. In this post, I'll compare the two index types to give users an idea of when to use each (\"json\" or view-based vs. \"text\" or search-based).\n",
      "----\n",
      "This video goes through a demo/workshop on how to build a Java EE App that using Cloudant and Watson to suggest employee recommendations. The app also using JQuery, Angular, and Bootstrap on the frontend. This app was internally developed by IBM employees at a 48 hour hackathon.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "Thinky is an open-source ORM designed for RethinkDB. Here, we show how to create schemas and define relations between your models.\n",
      "----\n",
      "One of the easiest strategies to help mitigate potential failures is to design your data model for idempotent functions, which can be correctly called over and over again, and to rely on a messaging system to deliver data at least once.\n",
      "----\n",
      "On June 6 we introduced the **IBM Data Science Experience** to the world at the Spark Maker Event that took place in Galvanize. We demonstrated the Experience with a real use case developed in\n",
      "----\n",
      "What a year it's been for Compose. Here are the highlights ...\n",
      "----\n",
      "The dplyr and tidyr packages are built to save you time when you wrangle data. Together, they provide a complete system for reshaping, transforming, and combining data sets.\n",
      "----\n",
      "A step-by-step guide to configuring Cloudant on Bluemix so that it is able to replicate to and from PouchDB - an in-browser, CouchDB-compatible database. PouchDB and Cloudant allow offline-first apps to be developed, allowing your app's users the ability to save their data even when not connected to the internet and syncing the data at a later date.\n",
      "----\n",
      "Looking to learn the basics of cloud databases? In this series, we show them running on Compose and intro programmatic access. First up: RethinkDB.\n",
      "----\n",
      "Interesting data science links from around the web, collected in Data Science Briefings, the DataMiningApps newsletter. \n",
      "----\n",
      "At Offline Camp, fellow IBM Developer Advocate Bradley Holt gave a Passion Talk on Cloudant Envoy. As I am more involved in this project than he is, Bradle\n",
      "----\n",
      "A run-down of Redis drivers for the most popular programming languages.\n",
      "----\n",
      "When used to make sense of huge amounts of constantly changing data, smart catalog capabilities can make all the difference.\n",
      "----\n",
      "Im going to show you how to deploy a live application to a Cloudant server and add the ability to use Twitter authentication. Future articles cover Offline First & Facebook auth.\n",
      "----\n",
      "This blog will explain what BigDL is and how it can be used in Data Science Experience (DSX).\n",
      "----\n",
      "This is an introductory post about using apply, sapply and lapply, best suited for people relatively new to R or unfamiliar with these functions.\n",
      "----\n",
      "A concise explanation of backpropagation for neural networks is presented in elementary terms, along with explanatory visualization. \n",
      "----\n",
      "See an easy way to upload files larger than 5GB to a Softlayer Swift cloud object store using IBM dashDBs moveToCloud script. \n",
      "----\n",
      "Bradley spends some time discussing the different types of NoSQL databases available and why you might choose one type over another.\n",
      "----\n",
      "Watch how a you can analyze dashDB data with R and publish insights with Shiny and dashDB.\n",
      "----\n",
      "In a previous tutorial, you saw how data flows could be run one after another by polling using a simple shell script. This tutorial demonstrates how to deploy the same functionality as a serverless\n",
      "----\n",
      "Every two weeks, we find the most interesting data science links from around the web and collect them in Data Science Briefings, the DataMiningApps newsletter. \n",
      "----\n",
      "Get faster queries and write less code too. Learn how to use Spark SQL to query your relational database. Follow this tutorial and see how to query a cloud-based Compose PostgreSQL instance or a local PostreSQL database.\n",
      "----\n",
      "If you are like most data scientists, you are probably spending a lot of time to cleanse, shape and prepare your data before you can actually start with the more enjoyable part of building and\n",
      "----\n",
      "An introduction to Bayesian Nonparametrics: the Dirichlet process along with associated models and links to their implementations.\n",
      "----\n",
      "More and more cloud-computing experts are talking about multicloud. The term refers to an architecture that spans multiple cloud environments in order to take advantage of different services\n",
      "----\n",
      "Caching a database can be a chore, but Mariusz Bojkowski shows how easy it can be to add a Redis cache to your PostgreSQL database if you are using Entity Framework 6.\n",
      "----\n",
      "Why YOLO is the better option compared to other approaches in real-time object detection.\n",
      "----\n",
      "Deep learning is a branch of Machine Learning that uses lots of data to teach computers how to do things only humans were capable of before. A good example of Deep Learning is perception, recognizing\n",
      "----\n",
      "Andy Ellicott and John David Chibuk talk about an Internet of Things application to record data captured from wearable technology and recorded in Cloudant \n",
      "----\n",
      "We show how to build a graph of airports and flight paths using GraphFrames. Then, visualize the data and apply various graph algorithms to analyze it.\n",
      "----\n",
      "Todays guest post is written by Vincent Warmerdam of GoDataDriven and is reposted with Vincents permission from blog.godatadriven.com. You can learn more about how to use SparkR with \n",
      "----\n",
      "PixieDust is a helper library for notebooks that makes it easier for teams of all types to work with data.\n",
      "----\n",
      "Customer use-case.\n",
      "----\n",
      "In this third entry in our Mongo Metrics series, we'll round out the \"top 3\" classical analytics methods by taking a look at mode.\n",
      "----\n",
      "Data exploration and analysis is a repetitive, iterative process, but  in order to meet business demands, data scientists do not always  have the luxury of long development cycles. What if data scientists  could answer bigger and tougher questions faster? What if they  could more easily and rapidly experiment, test hypotheses and  work more collaboratively on interactive analytics?\n",
      "----\n",
      "Recap of the SEIUM conference in Braga, Portugal, and its companion HeartBits hackathon. Offline-first Web apps were a big topic.\n",
      "----\n",
      "I spoke with Yingle Jia (Senior Software Engineer, IBM Verse and IBM Notes) about the offline capabilities recently added to IBM Verse, our web-based business email and calendaring software.\n",
      "----\n",
      "In this article, we'll explore MongoDB document validation by example using an invoice application for a fictitious cookie company. We'll look at some of the different types of validation available in MongoDB, and provide a practical working example of validations in action.\n",
      "----\n",
      "Cloudant Query provides you with a declarative way to define and query indexes. This video introduces you to Cloudant Query concepts.\n",
      "----\n",
      "Cloudant and its Apache CouchDB stable-mate are NoSQL databases  that is, they are schemaless JSON document stores. Unlike a traditional relational database, you dont need to define your schema\n",
      "----\n",
      "In Part 1, I gave you an overview of machine learning, discussed some of the tools you can use to build end-to-end ML systems, and the path I like to follow when building them.  In this post we are going to follow this path to train a machine learning model, deploy it to Watson ML, and run predictions against it in real time.\n",
      "----\n",
      "Take a peek at the future of data science in this discussion with five thought leaders in the data analytics industry, the second installment of a two-part interview recorded at the IBM Insight at World of Watson 2016 conference.\n",
      "----\n",
      "Interesting data science links from around the web.\n",
      "----\n",
      "Imagine youre interviewing a new job applicant who graduated top of their class and has a stellar rsum. They know everything there is to know about the job, and has the skills that your business\n",
      "----\n",
      "Apple and IBM announce they were providing a way to combine IBM Watson machine learning with Apple Core ML to make the business apps running on Apple devices all the more intelligent.\n",
      "----\n",
      "Sockets are the high power pipeline of the realtime web and in this article we'll show how a minimal amount of code can bring database data to life in a web browser.\n",
      "----\n",
      "We are pleased to announced that xml2 1.0.0 is now available on CRAN. Xml2 is a wrapper around the comprehensive libxml2 C library, and makes it easy to work with XML and HTML files in R. Install t\n",
      "----\n",
      "Cloudants NoSQL Database-as-a-Service clusters are hidden away in the depths of data centres around the world belonging to SoftLayer, Rackspace, Microsoft and Amazon, so there is little tangible product to display at a conference stand; no software to install, no drivers required, no sql! A working Cloudant cluster at the booth allows distributed databases to be seen in action with flashing lights indicating per-node activity.\n",
      "----\n",
      "A necessity in building an open source self-driving car is data. Lots and lots of data. We recently open sourced 40GB of driving data to assist the participants of the Udacity Self-Driving Car Challenge #2, but now were going much bigger with a 183GB release. This data is free for anyone to use, anywhere in the world.\n",
      "----\n",
      "The change from version 2 to 3 of the distributed etcd database also sees massive changes in how the database works. Let's understand the what and why of the changes.\n",
      "----\n",
      "MLDB is an opensource database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.\n",
      "----\n",
      "This video shows you how to replicate one of the sample databases on cloudant.com to your Cloudant account. \n",
      "----\n",
      "The New Builders podcast features weekly interviews with developers from around the web, discussing code, infrastructure, and their overall stack.\n",
      "----\n",
      "Here is this months collection of RStudio Tips and Tricks. Thank you to those who responded to last months post; many of your tips are included below! Be sure to subscribe to @rstudiotips on Twitter for more.This months tips fall into two categories: Keyboard Shortcuts and Easier R MarkdownKeyboard ShortcutsThe RStudio\n",
      "----\n",
      "Meet the new spark-cloudant connector, for adding powerful analytics to your Cloudant JSON. I also include a simple example that shows how to use SparkSQL to order Cloudant data by value.\n",
      "----\n",
      "The team at Compose who work on Redis looked at their recently introduced Slow Query Logs feature and decided they could make it better. In the process they created the new Redis Configuration Controls.\n",
      "----\n",
      "The IBM Data Science Experience (DSX) platform now integrates Streaming Analytics services using version 1.6 of the Python Application API, which enables application development and monitoring\n",
      "----\n",
      "Use Apache Spark, Cloudant, and Watson Tone Analyzer to perform sentiment analysis on a reddit Ask Me Anything web event.\n",
      "----\n",
      "In a previous post, we built an R Notebook that pulled in data on sector ETFs and allowed us to calculate the rolling correlation between a sector ETF and the S&P 500 ETF, whose ticker is SPY. Today, well wrap that into a Shiny app that allows the user to choose a\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "Compilation of Youtube videos teaching Statistics using R and other languages\n",
      "----\n",
      "Compose is now available on Google's Cloud Platform. Being a different platform, the configuration for launching your first Compose Enterprise cluster is also different and in this article, we'll walk you through what you need to do to create your own database powerhouse in your private cloud.\n",
      "----\n",
      "Have you ever wanted to quickly visualize the results of CouchDBs built-in reduce functions for some quick feedback, without leaving the context of its handy dashboard? Recently, my colleague and\n",
      "----\n",
      "Be aware of the different statistical bias types is inevitable, if you are about to learn data science and analytics. Here are the most important ones.\n",
      "----\n",
      "How to use Bunyan to capture detailed logging of data migration runs through our Simple Data Pipes app.\n",
      "----\n",
      "Zapier is a service which allows you to create custom integrations among a variety of applications, including PostgreSQL. Below we'll look at a couple of examples for how you can do more with Compose PostgreSQL by integrating it with other tools via Zapier.\n",
      "----\n",
      "Love to work in Microsoft Excel? Watch how to connect to IBM dashDB as the data source for Excel, and how to import tables into a spreadsheet. \n",
      "----\n",
      "This video shows you how to set up connections to both Bluemix and external sources.\n",
      "----\n",
      "The Spark Summit 2016 took place on June 68 in San Francisco and it was a sold out event with more than 2,500 attendees. Not surprisingly, deep learning (DL) and artificial intelligence (AI) were\n",
      "----\n",
      "Find out how to talk to a Redis database with nothing more than echo and the netcat command and get a deeper understanding of why developers love Redis.\n",
      "----\n",
      "Learn how to connect the 1.6.1 release of CouchDB to Cloudant's recently open-sourced Lucene integration.\n",
      "----\n",
      "Let's explore the `\\crosstabview` command, which gives you the power to rearrange how your data is viewed without the difficulty of writing complex SQL queries.\n",
      "----\n",
      "We visited Nick Pilkington, DroneDeploy's CTO, to talk about their mapping, app and how they're using Compose.\n",
      "----\n",
      "Brunel Visualization now has thoroughly re-worked code to provide improved options for mapping data to color. These maps of Africa show the results.\n",
      "----\n",
      "Now that the dust has settled on Apache Spark 2.0, the community has a chance to catch its collective breath and reflect a little on what was achieved for the largest and most complex release in the project's history.\n",
      "----\n",
      "we'll look again at the crosstab function, focusing this time on the option that does not use category sql. We'll explain how and when (not) to use it. We'll also compare it to the option that does use category sql ...\n",
      "----\n",
      "How I solved a graph development issue with parallel Cloudant Index creation requests.\n",
      "----\n",
      "In this interview with Chris Winslett, Compose developer, we talk about why MySQL is on the Compose platform, what makes it different on Compose and how the Compose for MySQL beta is going.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "Slack's integration API allows external services to be plugged in with ease. Even if your service isn't listed in the off-the-shelf integrations, you can still push data to other HTTP services. This tutorial shows how a Slack 'slash command' can be configured to push data to a Cloudant or CouchDB database in a few easy steps.\n",
      "----\n",
      "There's a world of tools that make the Elasticsearch even more useful and accessible. In this article we'll look at some and show what you do to get them working with Compose's Elasticsearch deployments. \n",
      "----\n",
      "The receptive field is perhaps one of the most important concepts in Convolutional Neural Networks (CNNs) that deserves more attention from the literature. This post will introduce a new way to visualize feature maps in a CNN that exposes the receptive field information.\n",
      "----\n",
      "Understand how Cloudant database replication works\n",
      "----\n",
      "Exciting civic and open data projects discussed Saturday at NYC School of Data.\n",
      "----\n",
      "Joan Touzet talks about ten common misconceptions about CouchDB and lends insight into best practices and design patterns.\n",
      "----\n",
      "A curated list of the most cited deep learning papers (since 2012).  \n",
      "----\n",
      "In this third part of the series on building smart business chatbots, well use a JanusGraph-backed knowledge base to give our chatbot from part 1 and part 2 some utility.\n",
      "----\n",
      "Which modern dimensionality reduction algorithms are best for machine learning? We'll discuss their practical tradeoffs, including when to use each one.\n",
      "----\n",
      "Data Refinery allows you to shape, combine, and enrich data from a variety of different sources. With a recently introduced feature, data scientists can use join operations to blend data sets faster. Lets look at some of the join operations that are now supported:\n",
      "----\n",
      "Analytics and visualization often go hand-in-hand. One of the great things about notebooks such as IPython/Jupyter is that they provide a single interface to...\n",
      "----\n",
      "Compose's Big Bits is a new podcast to keep you in the loop about what's happening in the data, database and tech world.\n",
      "----\n",
      "Gigi Sayfan shows you how to access PostgreSQL two ways using Go's standard library and sqlx.\n",
      "----\n",
      "How we reduced modeling time by 80% with Data Science Experience.\n",
      "----\n",
      "Learning is the most important ability and attribute of a Intelligent System. A system which acquires knowledge by experience, trial-and\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "MySQL doesn't have a way to index JSON documents directly, but it has given us an alternative: generated columns.\n",
      "----\n",
      "We're now the IBM Watson Data Lab on Medium. More on what that means for our blog and our developerWorks site.\n",
      "----\n",
      "At DataLayer Conference, Compose welcomed Sivabalan Narayanan from LinkedIn. LinkedIn runs Ambry, an open-source geo-distributed object store built in-house, on hundreds of nodes spanning multiple data centers and is the source of truth for all media and immutable content.\n",
      "----\n",
      "couchimport - CouchDB/Cloudant import tool to allow data to be bulk inserted\n",
      "----\n",
      "The search index lets you create flexible queries on one or more field in the documents. This video shows you how to perform group, facet, and geo searches.\n",
      "----\n",
      "Wed like to discuss time series prediction with LSTM recurrent neural networks. Well tell you how to predict the future exchange rate behavior using time series forecasting.\n",
      "----\n",
      "In the previous part of this tutorial, we looked at the two types of graphical models, namely Bayesian networks and Markov networks. In this post, we will cover parameter estimation and inference, and look at another application.\n",
      "----\n",
      "You need your latest backup and you need it automatically? We can show you how to create an on-demand backup and retrieve it automatically using the Compose API and Node.js.\n",
      "----\n",
      "Learn more about machine learning, the process of building analytical models to automatically discover previously unknown patterns from data.\n",
      "----\n",
      "Few of us have enough time to read, and most of us already have depressingly deep stacks of material that we would like to get through. However, sometimes a random encounter with something interesting is all that it takes to regenerate enthusiasm. Just in case you are not going to get to a\n",
      "----\n",
      "Emojis are an increasingly-pervasive sub-lingua-franca of the internet. They capture meaning in a rich, concise manner  alternative to the 13 seconds of mobile thumb-fumbling required to capture the same meaning with text. Furthermore, they bring two levels of semantic information: their context within raw text and the pixels of the emoji itself.\n",
      "----\n",
      "  Learn how to build a behavioral profile model for customers based on text attributes of previously purchased product descriptions. With SciKit, a powerful Python-based machine learning package for model construction and evaluation, learn how to build and apply a model to simulated customer product purchase histories. In a sample scenario, construct a model that assigns music-listener profiles to individual customers, based on the specific products each customer purchases and the corresponding textual product descriptions.\n",
      "----\n",
      "In the first part of this series about building a Cloudant Envoy application that uses Facebook to provide authentication, I showed you how to: All of the above is just a means of serving out your\n",
      "----\n",
      "See how to use IBM dashDB as data source loading data into Pyspark and then generate data analytics using Pandas/Ipython interfaces. \n",
      "----\n",
      "IBM Watson offers a collection of REST APIs for creating, running, managing, and troubleshooting data flows to allow your applications to easily integrate with Data Refinery. A flow can read data\n",
      "----\n",
      "Introducing a simple & free ETL service for CouchDB that handles document filtering and transformation, to prepare for cleaner data warehouse loading tasks.\n",
      "----\n",
      "We'll be exploring mappings and data types some more in this article using the larger petitions dataset. Before we index it, though, we need to introduce one or two more advanced topics.\n",
      "----\n",
      "Lucero Del Alba takes a look at how to use PostgreSQL's filter clause, and streamline database imports using PostgreSQL's foreign data wrapper in this Compose Write Stuff article.\n",
      "----\n",
      "This video shows you how to manage an object storage instance through IBM Data Science Experience (DSX).\n",
      "----\n",
      "Pandas cheat sheets: collection of code snippets, tips and tricks for Pandas Python numerical library\n",
      "----\n",
      "Use CARTO and IBM open data sets to add maps to your python notebook analysis.\n",
      "----\n",
      "IBM Graph is an easy-to-use, fully managed graph database service for storing, querying, and visualizing data points, their connections, and properties. Watch this video to see an overview of IBM Graph.\n",
      "----\n",
      "Create an app that takes an RSS news feed, passes it through the Alchemy Language API, then saves the enhanced data to Cloudant for querying.\n",
      "----\n",
      "The power of IBM's Bluemix cloud platform is now able to seamlessly harness Compose's databases, making Compose-configured MongoDB, PostgreSQL, Redis, RethinkDB, Elasticsearch, RabbitMQ and etcd available to Bluemix users and their applications. \n",
      "----\n",
      "Using mlxtend to perform market basket analysis on online retail data set.\n",
      "----\n",
      "Brunel Visualization lets you easily re-purpose visualization techniques and replace data variables with similar statistical properties.\n",
      "----\n",
      "Customer video form Compose's DataLayer conference.\n",
      "----\n",
      "How could I share log output with my co-workers?  I came up with a simple solution: logshare. It's a Node.js app that you can install via npm.\n",
      "----\n",
      "Covering both mobile and Internet of Things (IoT) use cases, this deep dive into offline first explored several patterns for using PouchDB together with Cloudant, including setting up one database per user, one database per device, read-only replication, and write-only replication.\n",
      "----\n",
      "Reflecting on my career, and advice on getting started\n",
      "----\n",
      "This video is a quick overview of the many ways you can create Jupyter notebooks powered by R, Python, or Scala in IBM Data Science Experience. Visit IBM Dat...\n",
      "----\n",
      "Deep Learning powered AI systems comes with complex difficulties and hurdles. In this post, we discuss prominent challenges in Deep Learning.\n",
      "----\n",
      "This paper explains why deep learning can generalize well, despite large capacity and possible algorithmic instability, nonrobust- ness, and sharp minima, effectively address- ing an open problem in the literature. \n",
      "----\n",
      "Node-RED is great for power prototyping, but how do you keep the bad guys (or the general public for that matter) from using your endpoints without permission?\n",
      "----\n",
      "This post has a series of videos and articles to help you go from getting started with Streams Designer to creating your own applications.\n",
      "----\n",
      "In this article, we'll be covering Elasticsearch and its Geo mapping datatypes, geo_point and geo_shape, and Geo querying capabilities. We'll show you how to construct your mappings and demonstrate how to query some data.\n",
      "----\n",
      "We'll look at the crosstab function in PostgreSQL to create a pivot table of our data with aggregate values.\n",
      "----\n",
      "See how to populate data into a table in your IBM dashDB database from a file located in a Softlayer Swift cloud object store. \n",
      "----\n",
      "Meteor Toys is the original development tools for the Meteor web dev framework. It covers everything from visualizing data to switching accounts in seconds.\n",
      "----\n",
      "Our thirty seventh release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "The secondary index provides a way for you to query data in more ways than just a simple primary key lookup. This video shows you how to create a secondary index, and construct queries to access the secondary index through the API. \n",
      "----\n",
      "I had seen a lot of work like this, using data science to prove that an existing map had been egregiously gerrymandered. But I had seen less work using data science to draw an optimally fair map. The challenge with drawing an optimally fair map, however, is that reasonable people disagree about what makes a map fair. Some believe that a map with perfectly rectangular districts is the most common sense approach. Others want maps optimized for electoral competitiveness  gerrymandered for the opposite effect. Many people want maps that take racial diversity into account. For example if a sate is 20% hispanic, 20% of its congressional districts should be majority hispanic.\n",
      "----\n",
      "An environment defines a relationship between a tool, software configuration, and hardware configuration. For example, you can create a Jupyter notebook environment configured with Anaconda 5.0 with 4 vCPU and 16 GB RAM.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "This white paper introduces SparkR, a package for the R  statistical programming language that enables programmers  and data scientists to access large-scale in-memory data  processing. The R runtime is single-threaded and can  therefore normally only run on a single computer, processing  data sets that fit within that machines memory. By providing  a bridge to Sparks distributed computation engine, SparkR  enables large R jobs to run across multiple cores within a  single machine or across nodes in massively parallel clusters,  with access to all the memory in the cluster. \n",
      "----\n",
      "So youve got your Fitbit over the Christmas break and youve got some New Years Resolutions. You go online and see the graphs on your dashboard but youre still not pleased. You want more data, more graphs, and more information. Well say no more, because Im going to teach you how to collect your own Fitbit data using nothing but a little Python code.\n",
      "----\n",
      "R is a flexible programming language designed to facilitate exploratory data analysis, classical statistical tests, and high-level graphics. With its rich and ever-expanding library of packages, R is on the leading edge of development in statistics, data analytics, and data mining. R has proven itself a useful tool within the growing field of big data and has been integrated into several commercial packages, such as IBM SPSS and InfoSphere, as well as Mathematica. This article offers a statistician's perspective on the value of R.\n",
      "----\n",
      "Data science is a team sport. This sentiment rings true not only with our experiences within IBM, but with our enterprise customers, who often ask us for advice on how to structure data science teams within their own organizations.\n",
      "----\n",
      "Were excited today to announce sparklyr, a new package that provides an interface between R and Apache Spark. Over the past couple of yearsweve heard time and time again that people\n",
      "----\n",
      "nan\n",
      "----\n",
      "This video shows you how to analyze data with R using a Shiny application in IBM Data Science Experience (DSX).\n",
      "----\n",
      "Earlier this month, I had the exciting opportunity to moderate a discussion between Professors Yann LeCun and Christopher Manning, ti...\n",
      "----\n",
      "It's time for our next DataLayer Conf video installment. This week, we'll hear about surviving failure with RabbitMQ from Lorna Jane Mitchell.\n",
      "----\n",
      "With a switch to newer MongoDB drivers and other low level improvements, the new Meteor 1.4 works much better with Compose's MongoDB.\n",
      "----\n",
      "How can a retail bank maximize expected revenue from a marketing campaign directed at new customers and using a limited budget? The Jupyter Notebook described in this post presents a combination of\n",
      "----\n",
      "Cloudant's Dan DeMichele introduces Cloudant Query, a declartive query language for NoSQL data, based on MongoDB's query language.\n",
      "----\n",
      "Our Location Tracker example app shows how simple it is to use Cloudant to track, store and query GeoJSON. It supports offline first design & future scale.\n",
      "----\n",
      "In this series of tutorials, well showcase the combined strengths of DSX Cloud and DSX Local with Watson Machine Learning. \n",
      "----\n",
      "The following information will help IBM Bluemix Data Connect consumers of the Data Load REST API to port to the IBM Watson Data API data flows service. At a high level, the activity JSON and data\n",
      "----\n",
      "Our thirty sixth release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "While AWS's Lambda service treats each function as an independent unit, Apex provides a framework which treats a set of functions as a project.\n",
      "----\n",
      "Finding clusters is a powerful tool for understanding and exploring data. While the task sounds easy, it can be surprisingly difficult to do i...\n",
      "----\n",
      "In this blog, were going to add some more npm modules into our notebook and show how you can create your own visualisations using only Node.js code. There are two more Node.js pixiedust_node helper\n",
      "----\n",
      "After shoveling the driveway several times and burning through the Netflix que, one way to counter act cabin fever is to hunt down some snowfall data and play around with it.  So, I found some data over at the National Weather Service that contains snowfall depth measurements collected from a variety of sources around the region at various time points during the storm.\n",
      "----\n",
      "When you create an account in IBM Data Science Experience we provision a free Apache Spark Cluster and 5 GB of Free IBM Object Storage. Some of our users shared that they are having trouble loading\n",
      "----\n",
      "FATHOM is an advanced manufacturer who likes challenges, like building the first 3D printed incubator or efficiently harnessing the collective power of professional 3D printers on its manufacturing floors.\n",
      "----\n",
      "Bradley Holt presents a webinar where he explores how we transformed IBM Cloudant's Location Tracker application from a CouchApp into a full three-tier Node.js app. Youll learn how to add user management functionality and leverage Cloudant Query to display a map of individual user locations within the app\n",
      "----\n",
      "How to store conversation data as a graph to add a recommendation feature to our chatbot example app. To the code!\n",
      "----\n",
      "Data Science Experience (DSX) is a complete platform for learning, building and collaboration. There are different features to enhance your learning experience. Lets explore some of the important\n",
      "----\n",
      "Cloudant Sync allows you to create native mobile apps for iOS and Android that can store data locally and sync with Cloudant when online. This allows your mobile application continue to work offline to deliver a great user experience.\n",
      "----\n",
      "SystemML includes makes it easier to express machine-learning algorithms and provides automatic optimization ensure both efficiency and scalability.\n",
      "----\n",
      "Im planning to release ggplot2 2.2.0 in early November. In preparation, Id like to announce that a release candidate is now available: version 2.1.0.9001. Please try it out, and file anissue on \n",
      "----\n",
      "Transporter 0.3.0 is out and it has a whole new way of working which will streamline its use on the mission to make moving data a joy, not a chore.\n",
      "----\n",
      "Watch interactive and self-service data analytics and visualization using dashDB together with Watson Analytics. \n",
      "----\n",
      "If you want to solve some real-world problems and design a cool product or algorithm, then having machine learning skills is not enough. You would need good working knowledge of data structures. The\n",
      "----\n",
      "See how a simple Python notebook, used in combination with a dashDB data warehouse and Apache Spark, can provide valuable insights into large data problems.\n",
      "----\n",
      "It is easy as an interviewee to bash the entire interviewing process  it takes forever, its painful, and it can be demoralizing. Just as well, its easy for interviewers to lament the difficulty in\n",
      "----\n",
      "Learn details of text analytics to see how this relatively mature but exciting form of analysis generates significant business value for a wide range of enterprises.\n",
      "----\n",
      "Using Python, and other tools, for natural language processing, sentiment analysis, and data wrangling.\n",
      "----\n",
      "Build models that learn over time with Watson Machine Learning.\n",
      "----\n",
      "See how to load new geospatial data into your dashDB database, and how to analyze it with Esri ArcGIS for Desktop. \n",
      "----\n",
      "In part one, we introduce the concept of a document conflict, describe what it looks like, and explain what happens if conflicts are left unresolved. Later in this series, we show how to tidy up conflicts, and discuss how they can be avoided.\n",
      "----\n",
      "Cloudant's replication allows you to replicate your data across Cloudant clusters to provide a global \"CDN\" for your database. This blog post shows how this can also power the offline-first design pattern, to store data locally first and sync to the cloud when there is an internet connection.\n",
      "----\n",
      "In this second half of MongoDB by Example, we'll explore the MongoDB aggregation pipeline. The first half of this series covered MongoDB Validations by Example.\n",
      "----\n",
      "What a difference a version number makes! With the release of Spark 2.0 (with Spark SQL) on July 26th, Spark SQL capability and performance has improved impressively.\n",
      "----\n",
      "The purpose of AI systems is to augment human intelligence, and today, we are excited to announce the next step on our journey to make AI more accessible for everybody with IBM Watson Studio.\n",
      "----\n",
      "Updated Transporter tool has a quicker way to get started, a completely rebuilt Elasticsearch adaptor, updated MongoDB adaptor, a new PostgreSQL adaptor, and lots of internal engineering improvements.\n",
      "----\n",
      "IBM Bluemix is based on Cloud Foundry. While its cf push command is a great way to deploy from local to cloud, here Ill show you a safer way for your team to automate its deployment pipeline.\n",
      "----\n",
      "For those familiar with the Apache CouchDB ecosystem, Cloudant Envoy is a microservice that serves out your static application and behaves as a replication target for your one-database-per-user\n",
      "----\n",
      "Looking to learn the basics of cloud databases? In this series, we show them running on Compose and intro programmatic access. Enter: Cloudant.\n",
      "----\n",
      "Since launching Data Science Experience in June of 2016, weve received a lot of feedback from data scientists. One piece of feedback we heard frequently was that DSX was great for enabling\n",
      "----\n",
      "We talked with three of our customers who are working in this industry on different yet compelling causes.\n",
      "----\n",
      "Amy Tai from Princeton University took the stage to discuss Replex and rethinking how to index in the modern, distributed datastore.\n",
      "----\n",
      "Todays world of data science leverages data from various sources. Commonly, these sources are Hadoop File System, Enterprise Data Warehouse, Relational Database systems, Enterprise file systems, etc\n",
      "----\n",
      "In the first 10 posts I mostlyconcentratedon theoretical topics. But the general focus of this blog is much broader.For the first time Im going to show an actual application of probability theory for estimating real life events. An ongoing event manypeople are closely following right now is the US presidential election. The primary season officially \n",
      "----\n",
      "In this article, you will learn how to bring data into Rstudio on DSX from Amazon S3 and write data from Rstudio back into Amazon S3 using sparklyr to work with spark and using aws.s3 to work\n",
      "----\n",
      "Ken Whipday shows how RabbitMQ has helped Tripcatcher. He'll show how to set up a basic AMQP message queue with NodeJS and then move on to Rabbot's powerful abstraction for RabbitMQ clusters, exchanges and queues.\n",
      "----\n",
      "In our Metrics Maven series, Compose's data scientist shares database features, tips, tricks, and code you can use to get the metrics you need from your data. In this article we'll cozy-up to calculating a median in PostgreSQL.\n",
      "----\n",
      "Use Spark Streaming in combination with IBM Watson to perform sentiment analysis and track how a conversation is trending on Twitter.\n",
      "----\n",
      "There are several reasons one might need to adjust the Spark configuration. For example, many machine learning libraries require Kryo Serialization. I recommend creating a whole project specifically\n",
      "----\n",
      "In our last article, we learned about using window functions in PostgreSQL. With that as our foundation, we'll dive a little deeper here to learn about window frames. Understanding window frames is important because the results you get from your window functions depend on how your windows are framed.\n",
      "----\n",
      "Our forty second release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "Developers new to Cloudant will welcome this list of guides and tutorials for how to get started with Cloudant.\n",
      "----\n",
      "Compose Elasticsearch and RabbitMQ deployments are being offered an upgrade to easier, more reliable Let's Encrypt security certificate-backed connections.\n",
      "----\n",
      "Try as we might, sometimes our projects dont fit nicely into notebooks. Notebooks are nice for R&D, but at some point you may need to build a more integrated folder structure for your project. One\n",
      "----\n",
      "When you deploy a new RethinkDB deployment or update RethinkDB to version 2.3, you need to know about RethinkDB's new user authentication system.\n",
      "----\n",
      "SystemML on Spark Shell? Yes!  A very simple way of using SystemML for all of your machine learning and big data needs. This tutorial will get you set up and running SystemML on the Spark Shell like a star. \n",
      "----\n",
      "For any software development organization, the cost of defects verification is extremely large. Such process is not always trivial or even achievable and often requires following very specific use\n",
      "----\n",
      "I'm happy to announce that we're working with our friends at Hoodie, Make&Model, and Bocoup to bring you the first ever Offline Camp!\n",
      "----\n",
      "Emulating filtered replication using the changes feed and OpenWhisk functions.\n",
      "----\n",
      "Data products have always been an instrumental part of Airbnbs service. However, we have long recognized that its costly to make data products. For example, personalized search ranking enables\n",
      "----\n",
      "This short video demonstrates how to use the maker palette in the IBM Data Science Experience to find content quickly and be more productive. Try it at http:...\n",
      "----\n",
      "This video provides an overview of IBM Data Catalog, part of Watson Data Platform. \n",
      "----\n",
      "In his latest Write Stuff article Robert Wysocki explains PostgreSQL backups, how to pick which ones to do and what the pros and cons are.\n",
      "----\n",
      "Cloudant Geospatial lets you run Nearest Neighbor queries to locate items close to a specific location.\n",
      "----\n",
      "For users wanting to improve performance by caching table data into memory, we offer some considerations.\n",
      "----\n",
      "Our goal wasnt to build the best event concierge app, but to illustrate the type of thing you could build with the cognitive services on the IBM Cloud.\n",
      "----\n",
      "Current version of the release notes, showing what new features will be added.\n",
      "----\n",
      "My favourite retro game! Demo of real-time notifications between web apps. Uses our Simple Notification Service npm module + RethinkDB for real-time connection between the client and server processes.\n",
      "----\n",
      "Watch how to create tables in IBM dashDB. \n",
      "----\n",
      "Part 3 uses gnuplot to graph data both interactively and programmatically via scripts. Plus, psql is also used to pull and dump data in the correct format.\n",
      "----\n",
      "A short guide on features of Python 3 for data scientists.\n",
      "----\n",
      "When you sign up for Data Catalog, you have a choice of two plans: Lite or Professional. Check out the key highlights.\n",
      "----\n",
      "The IBM Data Science Experience is a one stop shop for data scientists to learn, create and collaborate. In this video, Armand Ruiz Gabernet, the Data Scienc...\n",
      "----\n",
      "Why use R with Relational Databases?\n",
      "----\n",
      "David Taieb and Jorge Castan built and refined a sample app that predicts flight delays based on weather, using a data movement tool and Apache Spark.\n",
      "----\n",
      "The power of software to help us understand the world is immense and companies like DroneDeploy are bringing that power to everyone; their goal is to make the sky more accessible and productive for anyone. That includes the farmer looking for better crop yields, the architect looking to restore a cathedral or the miner wanting to optimize his excavations. \n",
      "----\n",
      "Learn how CouchDB 2.0's read and write operations behave in a distributed cluster. Cross-posted from Mike Rhodes at dx13.co.uk.\n",
      "----\n",
      "How I updated my sample chatbot to use the latest in graph databases.\n",
      "----\n",
      "To conquer their data layer, Alec Summers, CTO of iCars, chose Compose. So we decided to ask him why. Here's what he said.\n",
      "----\n",
      "A Python Flask RESTful web app that serves files out of your IBM Object Storage without requiring authentication.\n",
      "----\n",
      "Lucero Del Alba illustrates the power of PostgreSQL's aggregate and date functions to analyze financial data, and shows you how to display the results in a web browser using TechanJS.\n",
      "----\n",
      "Watch this video to see how to get started with IBM Watson Machine Learning (WML) by provisioning the necessary services in IBM Bluemix.\n",
      "----\n",
      "How to programmatically keep your search index up-to-date for JSON data that change over time. Using CouchDB/Cloudant and the Simple Search Service for quick faceted searches on your website.\n",
      "----\n",
      "Looking to learn the basics of cloud databases? In this series, we show how to use them on Compose and the IBM cloud. Enter: Redis.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "Add an aggregator microservice to a microservices app and use it to perform simple streaming analytics.\n",
      "----\n",
      "Cloudant Query gives you a declarative way to define and query indexes on your database. Based on MongoDB's query language, Cloudant Query makes it simple to query your data using simple operators. \n",
      "----\n",
      "This video shows you how to load Db2 Warehouse on Cloud data in a Scala notebook using Spark in IBM Data Science Experience (DSX).\n",
      "----\n",
      "Machine learning explores the study and construction of algorithms that learn and make predictions based on data. In the field of machine learning, data scientists, who specialize in analyzi\n",
      "----\n",
      "In this article, we take a look at how some of the marketing tech companies that work with Compose conquer their data layer\n",
      "----\n",
      "Were very excited to announce the general availability of Data Refinery, our self-service data preparation tool. We got a lot of feedback over the last 6 months that helped us tremendously to\n",
      "----\n",
      "When to use the following queue and broker technologies to handle app workload: JavaScript async library, Redis, RabbitMQ, Apache Kafka (IBM Message Hub)\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "In this article, we won't be taking a deep dive into indexing and virtual columns; we'll be focusing on keeping it simple by creating a table that stores some JSON documents that include a list of game players and the games they played.\n",
      "----\n",
      "I was dismayed to hear the news of North Carolina's anti-LGBT bill known as HB2. There were many thoughts that ran through my head, both personally and professionally.\n",
      "----\n",
      "This video shows you how to change database permissions for a Cloudant database.\n",
      "----\n",
      "Compose allows customers to deploy databases in many datacenters across the world with our commitment to provider and location diversity. Because of this flexibility, we need to take into account data privacy laws not just in the United States, but in every country where we store data.\n",
      "----\n",
      "Introducing our unofficial, shallow wrapper library for IBM Graph that makes its REST API even easier to use. Module ibm-graph-client on npm.\n",
      "----\n",
      "Modifying Database Data with R\n",
      "----\n",
      "Machine learning is fundamentally changing the way we approach computing  and it can pay off big time for your business.\n",
      "----\n",
      "Bluemix Object Storage service is preparing for multiple region support. Your Precipitation Analysis Sample Notebook may need an update.\n",
      "----\n",
      "This article walks you through a simple example of how to use Composes DBaaS offerings within IBM Bluemix, IBMs developer cloud platform. \n",
      "----\n",
      "This talk at PHP[tek] caught my eye because it covered more than just code. Amanda Folson is an engineer but also works in Developer Relations, so shes been a consumer of APIs and a visible\n",
      "----\n",
      "Not all code is built the same. The shelf life of your code typically depends on your motivations for building it: Ideally, all of our code is built to the highest professional standards, but we\n",
      "----\n",
      "Every two weeks, we find the most interesting data science links from around the web.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "This post discusses a new data-driven approach to predicting the outcome of NBA games. I call this the Elastic NBA Rankings. If you love statistics, R and basketball, then this is the post for you.\n",
      "----\n",
      "Learn Why and How To Use Spark for large amounts of data that requires low latency processing that a typical Map Reduce program cannot provide. Learn to use ...\n",
      "----\n",
      "This recipe shows how to use the IBM Data Science Experience tool to detect anomalies in historical timeseries data and create rules in IBM Watson IoT Platform based on these anomalies.\n",
      "----\n",
      "Artificial Neural Network (ANN) uses the processing of the brain as a basis to develop algorithms that can be used to model complex patterns and prediction problems.\n",
      "----\n",
      "Today, we're excited to introduce you to the new Compose Elasticsearch data browser that makes your Elasticsearch deployment even easier to manage from the web. You now have immediate access to the Elasticsearch API without leaving the Compose Console.\n",
      "----\n",
      "In this article, we discuss how to convert shapefiles into SQL and GeoJSON and import them into Compose PostgreSQL and MongoDB.\n",
      "----\n",
      "Dig into this breakdown of Hadoop components to gain an understanding of just how flexible the open source Hadoop framework is for performing big data analytics.\n",
      "----\n",
      "Learn how to connect applications that come with your dashDB fully managed data warehouse service in the cloud. \n",
      "----\n",
      "The combined release of the latest IBM Streaming Analytics service on IBM Bluemix and version 1.6 of the Python Application API introduces a suite of Python language features that greatly ease the development of streaming applications in the cloud.\n",
      "----\n",
      "A guide to some of the tools you can use to interact with Cloudant when using the Node.js programming language.\n",
      "----\n",
      "This video covers some more advanced techniques for using Cloudant secondary indexes.\n",
      "----\n",
      "Machine  learning is changing not only how we interact with machines,  but how we relate to the world around us. During the past  decade, machine learning has given us self-driving cars, speech  recognition, effective web search and a vastly improved  understanding of the human genome.\n",
      "----\n",
      "This video shows you how to connect your IBM Data Science Experience (DSX) account with your GitHub account.\n",
      "----\n",
      "The purpose of this article is to lay the groundwork for classical gradient boosting, intuitively and comprehensively.\n",
      "----\n",
      "The PostgreSQL team at Compose have been busy working on ways for you to get more control over your PostgreSQL deployments and that work is now available for you to use. The new Performance view lets you see how your tables and indexes are doing while the Extensions view lets you see what extensions are available to install and what's already installed. \n",
      "----\n",
      "Beautiful graphs in notebooks are great, but I want my explanatory text to look good too! Somehow I cant remember all the Markdown tags, so I created this cheatsheet. Headings: Use #s followed by a\n",
      "----\n",
      "Learn how to access the enterprise class dashDB service on Bluemix. Walk through these steps to create a simple chart-based application that uses the dashDB service and deploy it on Bluemix.\n",
      "----\n",
      "The need to access and analyze both internal and external data sources often poses practical problems that impose a significant drag on analytics efforts.\n",
      "----\n",
      "An overview of the most exciting highlights and research directions in optimization for Deep Learning in 2017.\n",
      "----\n",
      "Norman Barker & Chris Glew talk about using Cloudant to create geospatial applications for shipping, logistics and transportation.\n",
      "----\n",
      "Predict customer churn using deep learning with Keras to produce an Artificial Neural Network (ANN) model on the IBM Watson Telco Customer Churn Data Set.\n",
      "----\n",
      "See how to evaluate and convert your DDL and SQL to dashDB. \n",
      "----\n",
      "Running in the cloud is great as you get resources when you need them shared from a common pool of resources. For critical enterprise workloads though, you want to be selfish.\n",
      "----\n",
      "In this article, we will start designing a more complex trading strategy, which will have non-constant weights wi(t)wi(t), and thus\n",
      "----\n",
      "SparkR provides a distributed data frame API that enables structured data processing with a syntax familiar to R users.\n",
      "----\n",
      "Learn the basics of how to use CouchDB in PHP applications, using the excellent PHP HTTP library Guzzle.\n",
      "----\n",
      "In this article we're going to run some queries on our nested fields.\n",
      "----\n",
      "Amy Unruh gives an overview of Cloud Spanner, Google's relational and scalable application database.\n",
      "----\n",
      "Hoodie is a fast, simple and self-hosted backend as a service for your (web) apps. In short, Hoodie makes building Offline First apps a breeze. Plus, the Hoodie team is committed to doing wonderful\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "Interesting data science links from around the web.\n",
      "----\n",
      "Our thirty fifth release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "Python for loops are for iterating through sequences like lists, strings, dictionaries or ranges. In this article, Ill show you everything you need to know about them: the syntax, the logic and best practices too!\n",
      "----\n",
      "For those starting out in the field of ML, we decided to do a reboot of our immensely popular Gold blog The 10 Algorithms Machine Learning Engineers need to know.\n",
      "----\n",
      "If you were building an IT system to collect large-scale public petitions, you would have no idea how much computing power youd need to make the site performant. Serverless is compelling here.\n",
      "----\n",
      "Etcd is a fine database for coordination and configuration data but, at least for backup and restore, there's some design assumptions made that aren't a good fit for a hosted service.  That's why we were rather pleased to come across the etcdtool utility.\n",
      "----\n",
      "Find out how to use the power of the command line to control your Compose database deployments. The Bach tool is an easy way to harness the mighty Compose API.\n",
      "----\n",
      "Discover how a few considerations of a specific data set in a real-world use case enables data scientists to implement cost-effective data visualizations.\n",
      "----\n",
      "AI, machine learning, and deep learning are terms that are often used interchangeably. But they are not the same things.\n",
      "----\n",
      "The SETI@IBMCloud project and the SETI Institutes Hackathon and Code Challenge rely on IBM infrastructure, such as the Cloud Foundry Go runtime, Apache Spark and OpenStack Object Storage, all of\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "We've heard from some Compose users that connecting to MongoDB with Java and SSL is a somewhat difficult process so we're going to illustrate the process in a stripped down example.\n",
      "----\n",
      "Our forty sixth release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "Making proper use of connection pooling can massively improve the performance of your MongoDB deployment.\n",
      "----\n",
      "Creating a custom email server is a breeze with Postal and Compose. Take a deep dive in this article as we tackle all things email.\n",
      "----\n",
      "Watch how to generate SQL-based reports for Cloudant JSON data and dashDB with the Embeddable Reporting Service. \n",
      "----\n",
      "Data Science is a growing specialization that can touch on many of the following topics: Cloud computing, big data, math, business theory, and computer science theory. A scripting language like Python is often a great choice for the typical cycle of prototyping to make sure the math of the problem works, then \"productizing\" the result to a distributed farm of cloud servers. This article presents some hands-on examples of investment analysis and statistical analysis using IPython and pandas.\n",
      "----\n",
      "You may have heard about Graph databases but are they right for you? In this Write Stuff article, Graham Cox looks at the concepts and application of Graph databases.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. Dont forget to subscribe if you find this useful! Interesting Data Science Articles and News Managing Spark data handles in \n",
      "----\n",
      "This videos shows you how to create a very simple Python Flask web application front end to call a deployed Logistic Regression model in real time using IBM Waston Machine Learning and IBM Data Science Experience. This video is a continuation of the logistic regression analysis video.\n",
      "----\n",
      "Kaggle is a platform for predictive modelling and analytics competitions on which companies and researches post their data and statisticians and data miners from all over the world compete to produce the best model. One of the first problems that you face doing this kind of competitions is that the datasets are TOO big to handle with a regular computer. Not enough memorynot enough CPU power and the process is not performantSo thenwhat? I cannot participate? Or I have to pay expensive money to have a Cloud Environment? No! Use IBM Bluemix and DashDB!\n",
      "----\n",
      "Create full-text search indexes, built on Apache Lucene, with Cloudant Search. This step-by-step guide shows you how.\n",
      "----\n",
      "Cloudant is a highly available, partition tolerant distributed database and as such is 'eventually consistent', as per CAP Theorem. This blog outlines some techniques for dealing with eventual consistency in practice.\n",
      "----\n",
      "Spark Streaming batches are stateless by design: all transformations during a batch only affect the RDDs in that batch. As a result, applications are side effect free: running the same application an infinite number of times results in the same behavior and output. Similar to functional programming, this simplifies debugging and reasoning about the state of a program, as input and output paths are deterministic.\n",
      "----\n",
      "Life beyond the spreadsheet for CSVs\n",
      "----\n",
      "This video shows you how to quick start load a local data file and shape the data. \n",
      "----\n",
      "Building a data warehouse isn't as scary as it sounds. This example app lets you automatically load JSON into dashDB, IBM's a cloud data warehouse service.\n",
      "----\n",
      "An offline-first app that lets mobile utility workers access and update maintenance details and maps--even when disconnected.\n",
      "----\n",
      "Redis 3.2 brought with it a Redis Lua debugger, so let's have a quick look at it.\n",
      "----\n",
      "Unleash the power of data, analytics, and cognitive at IBM World of Watson 2016, October 2427 in Las Vegas. Explore the many Data Science related presentations and labs at World of Watson. Hear\n",
      "----\n",
      "Previously, you learned how to create your own custom visualization for PixieDust. The visualization code was written and tested directly in notebook cells, which is great for proof-of-concept and\n",
      "----\n",
      "How to scale your web app for success. Use microservices and IBM Message Hub to handle more users and a growing volume of data.\n",
      "----\n",
      "Its widely understood that organizations that embed analytics and data science into their operating models bring actionable knowledge into every business decision. Now these same businesses are\n",
      "----\n",
      "Weve added two new tools that make it even easier to learn Shiny. Video tutorial The How to Start with Shiny training video provides a new way to teach yourself Shiny. The video covers every\n",
      "----\n",
      "See how 28.io uses Compose MongoDB and PostgreSQL as part of their JSONiq demonstration videos. \n",
      "----\n",
      "Thinking of moving from etcd 2 to etcd 3? We're here to give you the why to move, the how-to do it and the what changes to expect when you do move.\n",
      "----\n",
      "Jupyter Notebook (a.k.a iPython Notebook) is brilliant coding tool. It is ideal for doing reproducible research. Here is my list of 10 tips on structuring Jupyter notebooks.\n",
      "----\n",
      "This video shows you how to create a project, associate the project with a data catalog, then add data from the data catalog to the project.\n",
      "----\n",
      "NiFi provides the services which are necessary and nice for a data flow tool plus it provides these services to an easily customizable class: the Processor. Here's how to use it with MongoDB.\n",
      "----\n",
      "A recap from OSCON Europe of Joey Lynch's excellent talk \"Building a Powerful Data Tier from Open Source Datastores\".\n",
      "----\n",
      "Thanks to great experimental work by several research groups studying the behavior of Stochastic Gradient Descent (SGD), we are collectively gaining a much clearer understanding as to what happens in the neighborhood of training convergence.\n",
      "----\n",
      "Visualization made easy with faith, trust, and PixieDust\n",
      "----\n",
      "dplyr 0.5.0 is a big release with a heap of new features, a whole bunch of minor improvements, and many bug fixes, both from me and from the broader dplyr community. In this blog post, youll find highlights of the most important changes.\n",
      "----\n",
      "Covering both mobile and Internet of Things (IoT) use cases, this deep dive into offline first will explore several patterns for using PouchDB together with Cloudant including setting up one database per user, one database per device, read-only replication, and write-only replication. Come prepared with your offline-first questions!\n",
      "----\n",
      "Learn about Bluemix's rich report-authoring environment. You can create visualizations, charts,formatted lists, and more. This tutorial shows you how to create a pedometer app on Liberty for Java that combines the dashDB, MongoLab, and Embeddable Reporting services on Bluemix\n",
      "----\n",
      "You can now run PostgreSQL 9.6 on Compose, PostGIS has been upgraded and now PGrouting is also available.\n",
      "----\n",
      "Learn how to set up a Cloudant replication job programmatically.\n",
      "----\n",
      "Need to move some data to the cloud for warehousing and analysis? Well walk you through your move from Oracle to a cloud-based dashDB warehouse. \n",
      "----\n",
      "Another analysis solution you can apply to Simple Data Pipe output. See how easy it is to move Cloudant JSON data on to a dashDB warehouse, then use built-in analysis tool R to glean meaning and insights.\n",
      "----\n",
      "We're continuing to bring you video of all the sessions from this year's DataLayer conference, and next up is Ross Kukulinski's talk on the state of state in containers.\n",
      "----\n",
      "This book is intended for anyone who works with or intends to develop Python applications such as application developers, consultants, software architects, data scientists, instructors and students. It is a good reference as well for dev ops, system administrators and product managers.\n",
      "----\n",
      "News of Esri's ArcGIS desktop, online, and open data offerings. How you can use this software in combination with Koop, Cloudant, and dashDB.\n",
      "----\n",
      "The ggplot2 package lets you make beautiful and customizable plots of your data. It implements the grammar of graphics, an easy to use system for building plots.\n",
      "----\n",
      "I love containers, and I have been playing with Kubernetes for a while now, so I was excited to see IBM offering Kubernetes as part of the IBM Bluemix Container Service (Note: as of this writing this\n",
      "----\n",
      "Cloudant's Traffic Tamer app collects car telemetry data. Every time a car stops, this event is synchronized with Cloudant to enable crowd-sourced traffic information. This post describes how PouchDB is used to collect data offline and sync to Cloudant when on the network.\n",
      "----\n",
      "Trust is a funny thing. One day youre on top of the world, the next you can do no right. The recent election has obviously shaken trust in the analytics establishment. While there are many reasons\n",
      "----\n",
      "Video from Compose's DataLayer conference.\n",
      "----\n",
      "Our forty fourth release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "In this article, you'll build a quick and easy signup form in Node-RED that allows users to enter their information and saves entered data to a Compose MongoDB database. We'll also look at validating that data, in this case an email address, and handling validation errors.\n",
      "----\n",
      "In this talk, long-time Compose user Vikram Tiwari of Omni Labs presents on how the company bootstrapped itself using Compose.\n",
      "----\n",
      "Develop an iOS app with Swift and IBM Cloudant. This post demonstrates UI elements for offline-first design, with pull-to-sync and replication spinners.\n",
      "----\n",
      "Basic concepts and intuition of using different kinds of machine learning algorithms in different tasks.\n",
      "----\n",
      "For the week ending 10th February: RethinkDB has a new home, Redis's future is being mapped out, PostgreSQL 10's features are appearing, and more.\n",
      "----\n",
      "Watch this video to see how to build a Naive Bayes model using IBM Waston Machine Learning and IBM Data Science Experience that assesses what category of goods a customer might be interested in.\n",
      "----\n",
      "Introducing silverlining, a Node.js library that hides away the powerful-but-esoteric functionality of Cloudant and concentrates on making data manipulation simple and intuitive for new users.\n",
      "----\n",
      "Working with Spark we will show data engineering operations that will shape and prepare data for analysis. This will include augmenting the existing data set...\n",
      "----\n",
      "Mongo Metrics is a new series in collaboration with Compose's Resident Data Scientist Lisa Smith that shows you how to extract insights and toy with data stored in Compose MongoDB. This series is a MongoDB flavor of our popular Metrics Maven series.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "Learn how to load Cloudant data in IBM Analytics for Apache Spark using the Cloudant-Spark connector in a Scala notebook.\n",
      "----\n",
      "This video shows you how to set up a pre-authenticated version of cURL to eliminate the need to type your username and password each time you execute a cURL command to act on a Cloudant database.\n",
      "----\n",
      "Learn how to use Bluemix Data Connect to load, mix, and refine raw data from both on-premises and off-premises environments.\n",
      "----\n",
      "The RStudio IDE is the most popular integrated development environment for R. Do you want to write, run, and debug your own R code? Work collaboratively on R projects with version control? Build packages or create documents and apps? No matter what you do with R, the RStudio IDE can help you do it faster. This cheat sheet will guide you through the most useful features of the IDE, as well as the long list of keyboard shortcuts built into the RStudio IDE.\n",
      "----\n",
      "We are happy to announce a new package DT is available on CRAN now. DT isan interface to the JavaScript library DataTablesbased on thehtmlwidgetsframework, to present rectangular R data objects\n",
      "----\n",
      "We've just added a new feature to the Bach command line for the Compose API to make things easier to track to completion. Let us explain what --watch and --wait does with advice for command line users and API developers too.\n",
      "----\n",
      "In this post we will look at two probability distributions you will encounter almost each time you do data science, statistics, or machine learning.\n",
      "----\n",
      "The failure to appreciate data governance may come from a lack of understanding about the value it can deliver and how important it is to future success.\n",
      "----\n",
      "Build the back-end for an iOS mobile app using Bluemix and Cloudant. Youll configure authentication with Facebook, implement cloud-based data storage that syncs with your mobile app, and set up push notifications too.\n",
      "----\n",
      "We asked five social influencers how data scientists can collaborate to build better business applications? See what they had to say.\n",
      "----\n",
      "It can be hard to visualize exactly how a new closet or shelving system would fit into your living quarters, which is why Organized Living turned to Compose customer, Differential, to build an easy-to-use 3D visualization tool.\n",
      "----\n",
      "In this article we're going to look at using Node to connect to an Elasticsearch deployment, index some documents and perform a simple text search.\n",
      "----\n",
      "I recently returned from Offline Camp Berlin, our third Offline Camp. A funny thing has happened at every Offline Camp to date. Inevitably someone will see the stream of tweets coming out of Offline\n",
      "----\n",
      "In my last blog, we looked at moving data from Amazon DynamoDB to Cloudant or CouchDB. In this article were going to look at extracting data from Microsoft Azures DocumentDB service. The tool makes\n",
      "----\n",
      "In the final step of the series, we create a web application to access the Elasticsearch managed data and show how to host the web app on IBM's Bluemix.\n",
      "----\n",
      "LodgIQ CTO Somnath Banerjee is bringing dynamic data science and machine learning to the hospitality industry through a platform built on Apache Spark.\n",
      "----\n",
      "In my previous life I was a particle physicist, searching for very rare nuclear reactions (in order to count neutrinos from the sun and search for dark matter). I had forgotten, until very recently\n",
      "----\n",
      "This video shows you how to construct queries to access the primary index through Cloudant's API.\n",
      "----\n",
      "Combine your customer and sales data with open data sets like Census stats. Then perform analysis in the cloud to make data-driven business decisions.\n",
      "----\n",
      "One thing these diverse consulting firms all agree on is that theyve found the right mix of databases and managed database services at Compose.\n",
      "----\n",
      "Transporter was developed originally to synchronize MongoDB databases. In this article, we'll look at how to configure the latest generation of Transporter to do just that.\n",
      "----\n",
      "GeoFile is a series dedicated to looking at geographical data, its features, and uses. In today's article, we're going to show you how to convert OSM data to GeoJSON and import it into a Compose for MongoDB deployment.\n",
      "----\n",
      "Were proud to announce version 1.1 of the tibble package. Tibbles are a modern reimagining of the data frame, keeping what time has shown to be effective, and throwing out what is not. Grab the la\n",
      "----\n",
      "To get a grip on an organizations data, a CDO first needs to know what data the company has and who is using it - which isn't as simple as it seems.\n",
      "----\n",
      "Our forty third release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "When youre learning R and Rs Tidyverse packages, its important to break everything down into small units that you can learn. And once you have mastered the essential functions as isolated units, you need to put them together. With that in mind, well show you another small project that puts it all together.\n",
      "----\n",
      "The OReilly Data Show podcast: Eric Colson on algorithms, human computation, and building data science teams. [A version of this post appears on the OReilly Radar.] Subscribe to the O&#8217\n",
      "----\n",
      "Apple's Food Tracker sample app taught you iOS. Take it further and sync data through the cloud, with an offline-first design. Part 2 covers cloud JSON sync\n",
      "----\n",
      "Announcing tidyr 0.6.0. tidyr makes it easy to tidy your data, storing it in a consistent form so that its easy to manipulate, visualise and model. Tidy data has a simple convention\n",
      "----\n",
      "Flying Donut has created a simple and intuitive agile collaboration and project tracking tool running on Compose MongoDB and RabbitMQ.\n",
      "----\n",
      "At icanmakeitbetter, the mix of the Elixir language and Compose's MongoDB databases are key components of the stack that powers the company's market research platform.\n",
      "----\n",
      "Claudius Li and Barry Wark talk about how Life Science Software uses Cloudant to store biological and chemical experimental data.\n",
      "----\n",
      "Combine Apache Spark with other cloud services to speed analysis and reveal insights.\n",
      "----\n",
      "Data Science Experience (DSX) is a unified analytics environment providing access to Jupyter notebooks and RStudio on top of IBM Analytics for Apache Spark (think Spark-as-a-Service). DSX is tightly\n",
      "----\n",
      "Learn you how to analyze New York City traffic collision data using a Python Notebook in IBM Analytics for Apache Spark.\n",
      "----\n",
      "Learn how to create a connection to dashDB data in IBM Analytics for Apache Spark, and load data in a Scala notebook.\n",
      "----\n",
      "Graham Cox talks about integration testing against live databases on both local and Compose-managed deployments.\n",
      "----\n",
      "There's a number of datatypes available in PostgreSQL. In this article, we're going to take a look at the array datatype.\n",
      "----\n",
      "We're going to be talking about how you can run your own Kibana instance for Elasticsearch. You may think this is an odd thing to want to do, but all Elasticsearch users should be paying attention to this.  That's because Elasticsearch has been restructured over the last few versions, so some components have been moved away from the central server's plugin architecture and given their own server.\n",
      "----\n",
      "The black box of a random forest can be opened up by tracking decision paths along the trees and computing feature contributions. This way, any prediction can be decomposed into contributions from features. However, this linear breakdown is inherently imperfect, since a linear combination of features cannot capture interactions between them...\n",
      "----\n",
      "In this post I will show you how to use the IBM Analytics for Apache Spark service from the RStudio IDE, which is integrated into the Data Science Experience. RStudio is the premier integrated\n",
      "----\n",
      "Building a platform means building an API to go with it and building an API means managing the keys to that API efficiently. Kirk Morales, CEO at Hyver, has used ScyllaDB to solve this tricky problem.\n",
      "----\n",
      "We'll introduce you to Odoo, an open source ERP system, and show you how to deploy the Odoo stack on IBM Bluemix Container Service with Compose PostgreSQL providing the data layer.\n",
      "----\n",
      "The beauty of Apache CouchDB and Cloudant is that you dont need to a library to be able to start using it. Some databases require a driver module to be installed to handle communication between\n",
      "----\n",
      "When considering big data as a source for insight to enhance decision making, it may be best characterized by its three Csconfidence, context and choice.\n",
      "----\n",
      "Compose's open-sourced Transporter is a powerful tool. In this article, we'll show you how to use it to upload data to a MongoDB database using it and unlock the power of Transformers.\n",
      "----\n",
      "Every two weeks, we find the most interesting data science links from around the web and collect them in Data Science Briefings, the DataMiningApps newsletter.\n",
      "----\n",
      "Today, many companies use big data to make super relevant recommendations and growth revenue. Among a variety of recommendation algorithms, data scientists need to choose the best one according a\n",
      "----\n",
      "At Compose we like to have a quick peer into the JIRA issues repository and see what things could well be coming. The 3.3 branch has been in development since January and now, five versions in, we thought it would be a good time to have that look. Here are some of the things that caught our eye...\n",
      "----\n",
      "In this article, we demonstrate the Bayesian method to estimate the parameters of the simple linear regression (SLR) model.\n",
      "----\n",
      "Jupyter notebooks provide an effective way to develop, document, execute and communicate your results. Here are few tips and tricks to make your life easier while working on notebooks in Data Science\n",
      "----\n",
      "With serverless programming in general, and OpenWhisk in particular, actions run in response to an event. An event might be a web request, a database change, a sensor readingthe possibilities are\n",
      "----\n",
      "We explore the open-source block chain technology Ethereum and build a smart contract to help you understand the strengths of block chain platforms.\n",
      "----\n",
      "Weve encapsulated the HTTP API for the IBM Graph database service and made it available as an open source library to help you get up and running faster from Java. Check out our simple sample app!\n",
      "----\n",
      "ReadMe.io has created a simple, gorgeous platform for publishing technical documentation running on Node.js and Compose Enterprise MongoDB and Redis. Here's how they're using it.\n",
      "----\n",
      "Offline-first apps using Cloudant Sync deliver a better UXboth offline and onby storing data locally first, then synching to the cloud when connected.\n",
      "----\n",
      "Lets say you want to create a machine learning model to predict the price of homes in a particular city. You gather historical sales data, details about the homes sold, proximity to parks, income\n",
      "----\n",
      "Last month I announced the availability of PixieDust 1.0. Since then, community adoption has been fantastic. Based on repo stars and on feedback at conferences and events, more developers and data\n",
      "----\n",
      "As machine learning becomes more powerful, the fields researchers increasingly find themselves unable to account for what their algorithms know  or how they know it.\n",
      "----\n",
      "we're going to look at how to connect to Compose RethinkDB using Elixir and pick out some of the pitfalls.\n",
      "----\n",
      "This video introduces you to the different types of indexes that you can create to query the data in your database, and explains the typical use cases for each type of index. \n",
      "----\n",
      "Looking to learn the basics of cloud databases? In this series, we show them running on Compose and intro programmatic access. Enter: PostgreSQL.\n",
      "----\n",
      "Today were excited to announce htmlwidgets, a new framework that brings the best of JavaScriptdata visualization libraries to R.There are already several packages that take advantage of th\n",
      "----\n",
      "How to use Compose's Redis Data Browser\n",
      "----\n",
      "Developers coming to Cloudant from an SQL background will have lots of questions about importing data, schemas and authentication. This post answers the most frequently asked questions.\n",
      "----\n",
      "A summary of recent articles on chatbots by the Watson Data Platform and the Watson Developer Cloud dev advocacy teamsincluding a story originally published on Free Code Camps Medium publication.\n",
      "----\n",
      "This video shows you how to connect to an IBM Bb2 Warehouse on Cloud database in RStudio in IBM Data Science Experience (DSX).\n",
      "----\n",
      "At Compose, we understand how, with Redis as an essential cog in your operations, you'd want to minimise downtime when migrating. So, for our first offering, we've created a static import tool which will import your data from a remote Redis installation into your Compose Redis deployment.\n",
      "----\n",
      "The Cloudant/dashDB integration is officially out of beta and into General Availability. For those that dont know, IBM dashDB is a cloud-based data warehouse with built-in analytics tailored to help ensure youre making the most of your data. dashDB is accessible right in the Cloudant dashboard by way of the Warehousing tab.\n",
      "----\n",
      "This video shows you how to use the Cloudant dashboard to create a database and add documents to the database using the easy-to-use JSON editor. \n",
      "----\n",
      "With two of the best known MongoDB desktop UI's now under one roof, we decided to take a look at who should be using either of them - and how well they work with Compose.\n",
      "----\n",
      "Here are some stories from this week in Data Science and Big Data. \n",
      "----\n",
      "Sven Hafeneger of IBM delivered a lightning talk called Hyperparameter Optimization - when scikit-learn meets PySpark.\n",
      "----\n",
      "Learn how to manage replication jobs in Cloudant\n",
      "----\n",
      "Because it is optimized for analytic operations in many ways, Db2 Warehouse on Cloud (formerly known as IBM dashDB) is an excellent choice to be the home of your data to analyze with Data Science\n",
      "----\n",
      "The Cloudant/CouchDB dashboard-development team has just landed substantial UX improvements for data-replication.\n",
      "----\n",
      "Last Thursday (June 16th) marks my one-year anniversary of working at Stack Overflow as a Data Scientist.  Id finished my PhD about a month before I joined, and my move to a tech company was a pretty big change for me. As of only a few months earlier, Id been planning to stay in academic research, particularly in the field of computational biology. Id started applying for postdoctoral fellowships, and hadnt even considered applying to industry jobs.  What changed my mind?\n",
      "----\n",
      "See how to create a new dashDB instance and populate it with data directly from a Cloudant account. \n",
      "----\n",
      "In this article, we'll look at how to calculate a weighted average and apply that to a group of products from our hypothetical pet supply company.\n",
      "----\n",
      "How we used our own services to build a tool that tracks deployments of sample apps to IBM Bluemix.\n",
      "----\n",
      "RLS does just what it says: it secures a row in a table. But, you do have to enable it for each table plus you need to commit to using database roles as a main security mechanism. That last part is the barrier but also the reason to use such a feature.\n",
      "----\n",
      "Explore a highly effective deep learning approach to sentiment analysis using TensorFlow and LSTM networks.\n",
      "----\n",
      "How to improve the Watson Recipe Chatbot by integrating Cloudant to cache 3rd party API calls, provide a personalized UX, and run analytics on interactions.\n",
      "----\n",
      "how to analyze data in IBM BigInsights on Cloud using BigSheets and Big SQL\n",
      "----\n",
      "Get started building machine learning models with Python and H2O on IBM's Data Science Experience.\n",
      "----\n",
      "Logistic regression is a method for fitting a regression curve, y = f(x), when y is a categorical variable. The typical use of this model is predicting y given a\n",
      "----\n",
      "Import structured data into a Cloudant NoSQL database, where it is indexed and presented as a faceted search API.\n",
      "----\n",
      "Apache Mahout 0.13.0 just dropped- a huge release that adds support for Spark CPU/GPU acceleration via native solvers. Apache Mahout is a linear algebra library that runs on top of any distributed\n",
      "----\n",
      "Today, after months of development, we are pleased to be re-introducing Compose Enterprise. Compose Enterprise adds the option of using managed hosts that are dedicated to your own Compose cluster so you can have all the features of Compose along with encrypted data at rest and control over backups for compliance and security.\n",
      "----\n",
      "Computational notebooks provide a workspace where programmers and data scientists can process, visualise and analyse data on a web page. Usually, notebook code is written in Python, Java or Scala \n",
      "----\n",
      "In this article, well continue our discussion of getting locations within a radius, but apply it to GeoJSON documents and MongoDB. Well be setting up 2dsphere indices and using MongoDBs built-in geospatial query operators.\n",
      "----\n",
      "Now you can create streaming data topics in the Watson Data Platform UI, receive data from diverse sources, and create connections to these topics in order to analyze streaming data in your analytic\n",
      "----\n",
      "Here we review a couple scenarios of moving data using NiFi. First, we'll look at the simplest approach possible of just queries and inserts then a brute force approach of polling with de-duplication and then move on to a more advanced synchronization approach by hooking into MongoDB's oplog.\n",
      "----\n",
      "This video shows you how to create a connection to a data source in IBM Data Refinery and then add connected data to a project. \n",
      "----\n",
      "Compose MongoDB users haven't had to worry about the problem, but it is worth looking at what is going on and why it isn't a worry for them.\n",
      "----\n",
      "Watch this video to see how to create a project in IBM Data Science Experience (DSX) and set it up to use Watson Machine Learning (WML).\n",
      "----\n",
      "Recent achievements in deep learning over the past year (and a bit longer). Well tell you about the most significant developments that can affect our future.\n",
      "----\n",
      "At Compose, we believe databases should be as easy to use as any utility service you get at home. Thats why all databases on the Compose platform are managed services.\n",
      "----\n",
      "This blog deals shows you how to implement web analytics in Python.\n",
      "----\n",
      "For the last couple of months, weve blogged about the data challenges faced by many organizations today, particularly in areas of access, collaboration and governance. In those posts, we presented a\n",
      "----\n",
      "Learn how to set up Cloudant database replication job.\n",
      "----\n",
      "KDnuggets Editors bring you the answers to 20 Questions to Detect Fake Data Scientists, including what is regularization, Data Scientists we admire, model validation, and more. \n",
      "----\n",
      "This video shows you how to build a deep learning model using the popular CIFAR10 data set using IBM Watson Machine Learning Neural Network Modeler.\n",
      "----\n",
      "The Data Science practice is amazing and complex. A solo data scientist has to form a relevant hypothesis, find a corresponding data set, clean it, and repeatedly build and edit a model to prove or disprove their hypothesis.\n",
      "----\n",
      "Chris Erwin shares his experience with partial indexes in PostgreSQL and talks about this incredibly powerful tool many have never heard of.\n",
      "----\n",
      "Data Science is hard; doing it alone is harder. Check out two new features available in Data Science Experience to help make it easier to work as a team on your notebooks.\n",
      "----\n",
      "See how to connect dashDB, as a source and target database service in the cloud, and how easy it is to synchronize data.\n",
      "----\n",
      "Overview   Imitation learning, a.k.a behavioral cloning, is learning from demonstration. In other words, in imitation learning, a machine...\n",
      "----\n",
      "One of the most important skill for data scientists to have is the ability to clearly communicate results to a general audience. The impact of data scientists work depends on how well others can\n",
      "----\n",
      "70 free data sources for 2017 on government, crime, health, financial and economic data, marketing and social media, journalism and media, real estate, company directory and review, and more to start working on your data projects. \n",
      "----\n",
      "Learn how to use the Aggregation operator in Streams Designer to create applications that compute and store various statistics for streaming data.\n",
      "----\n",
      "A list of software libraries and tutorials for a variety of programming languages\n",
      "----\n",
      "We review 2 great tools for working with your databases.\n",
      "----\n",
      "In this blog post we will try to predict chronic kidney disease using various attributes collected from hospitals. Chronic kidney disease (CKD) is a condition characterized by a gradual loss of\n",
      "----\n",
      "The SETI Institutes upcoming hackathon and code challenge is based on a set of simulated data that will be used as a labeled training data set to build signal classification models. This post will\n",
      "----\n",
      "Work through examples of row-oriented work in data frames, explore data analysis patterns, discuss split-apply-combine, row-wise work in data frames, splitting vs. nesting, and list-columns.\n",
      "----\n",
      "The IBM Cloudant SDK team is proud to announce the release of Cloudant Sync, an iOS library that provides a simple storage API for your app.\n",
      "----\n",
      "The devtools package makes it easy to build your own R packages, and packages make it easy to share your R code. \n",
      "----\n",
      "Spark SQL Version 1.6 runs queries faster! Thats the good news from Berni Schiefer, an IBM Fellow, an STC engineer with deep expertisein rigorous performance testing, and a native of the great Ca\n",
      "----\n",
      "Learn about the latest Big Data University announcements and news from the world on Big Data on our blog.\n",
      "----\n",
      "The MRT Circle Line was hit by a spate of mysterious disruptions in recent months, causing much confusion and distress to thousands of commuters. Like most of my colleagues, I take a train on the\n",
      "----\n",
      "A developer, data scientist, or line-of-business user should be able to run a real-time analytics app, end-to-end, from within a single Python Notebook. Ill show you how, with the PixieDust library.\n",
      "----\n",
      "Working with notebooks in Data Science Experience just became easier with the release of two new features! First, we listened to your feedback and made it much easier to load JSON data from Object\n",
      "----\n",
      "Watch how to create a Bluemix application using Node-RED that searches for tweets and stores the resulting tweets in a Cloudant NoSQL database, then use the data warehousing tool built into Cloudant to load the data into a dashDB.\n",
      "----\n",
      "Today were excited to announce flexdashboard, a new package that enables you to easily create flexible, attractive, interactive dashboards with R. Authoring and customization of dashboards i\n",
      "----\n",
      "IBM Watson  offers a collection of REST APIs for creating, running, managing, and troubleshooting data flows to allow your applications to easily integrate with Data Refinery.\n",
      "----\n",
      "Watch this video to see how to use IBM Watson Machine Learning and IBM Data Science Experience to create a data flow using IBM SPSS Modeler to predict chronic ki...\n",
      "----\n",
      "When working with data, the format of the raw data is not always user-friendly. For instance, the format could be one large binary file, or the data could spread across hundreds of text files. An\n",
      "----\n",
      "A lot has changed in the almost fifteen years that Ive been building web apps. Ive always loved the open nature of the web platform. There are no gatekeepers on the web. No one can tell you what\n",
      "----\n",
      "We've boosted the reliability of Cache Mode on Compose by disabling backups and persistence by default on all Redis deployments using that mode. But if you need them enabled, all you need to do is turn it on.\n",
      "----\n",
      "Adding a powerful faceted search feature to your app or website is easier than you think. Quickly turn spreadsheet data into a polished search feature.\n",
      "----\n",
      "In this post, we will look at ways to connect to a Compose PostgreSQL deployment using R, and how we can run queries from our R development environment to Compose PostgreSQL.\n",
      "----\n",
      "For Beginners in R, here is a 15 page example based tutorial that covers the basics of R.\n",
      "----\n",
      "Although open source code in Python and R is popular because of its low cost, flexibility, and power, the time required to properly create code and ensure that it is working correctly can be\n",
      "----\n",
      "How to load raster data into dashDB for detailed analysis with Python and R.\n",
      "----\n",
      "This blog leverages a previous post that introduced work with Object Storage in Data Science Experience (DSX). In order to read the excel file into a Pandas DataFrame, you might need to install a\n",
      "----\n",
      "In this second entry in our new \"Mongo Metrics\" series, we'll take a look at using the MongoDB aggregations pipeline to compute the MEDIAN of a set of data.\n",
      "----\n",
      "In this video we can see Tom Krouper from GitHub as he introduces gh-ost and compares it with existing online schema change tools.\n",
      "----\n",
      "Learn how to use Chartio's BI platform with IBM's dashDB cloud data warehouse to build powerful dashboards with data sources like Stripe and Salesforce.\n",
      "----\n",
      "In this article we'll learn how to calculate a weighted moving average in PostgreSQL.\n",
      "----\n",
      "Graph 101 is an article series on graph databases that explores graph algorithms from the ground up. If youve ever wondered whether or not a Graph database is a good approach for a problem youre trying to solve, Graph 101 is the series for you.\n",
      "----\n",
      "Missing data is a common and exciting problem in statistical analysis and machine learning. They are necessary for evaluating data quality and can have different sources such as users not responding\n",
      "----\n",
      "Read how to access your on-premises relational database server from a cloud-based Bluemix app.\n",
      "----\n",
      "Create a contact management app that's stored in an Apache CouchDB database\n",
      "----\n",
      "Aginity Workbench is a free application known best for working with IBMs PureData for Analytics, formerly Netezza. With the launch of dashDB, Aginity released a new version of the workbench to provide a familiar interface for working with this cloud data warehouse. \n",
      "----\n",
      "It is always amazing when someone is able to take a very hard, present day problem, and translate it to one that has been studied for centuries. This is the case with Word2Vec, which transforms words into vectors. \n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "Cloudant will soon run on the Apache CouchDB 2.0 code base. Today, we're announcing a sandbox cluster for functional testing. Here's what's new.\n",
      "----\n",
      "This video shows you how to access data sets in IBM Data Science Experience.\n",
      "----\n",
      "The recently released Redis 3.2 now has an official Geo API in the mainstream branch of the in-memory database. If you are curious on how you could use this, read on...\n",
      "----\n",
      "Gareth Dwyer writes about using SQLAlchemy, a Python SQL toolkit and ORM, discussing the advantages of using it while performing database operations using PostgreSQL.\n",
      "----\n",
      "In Compose PostgreSQL you can now perform cross-database queries using some extensions we've recently made available: postgres_fdw and dblink. In this article we'll walk through how to set up your database to take advantage of them.\n",
      "----\n",
      "We're going to introduce you to OpenStreeMap data, import that data into a Compose PostgreSQL deployment, and make queries on some of the non-conventional data stored in hstore columns.\n",
      "----\n",
      "PostgreSQL 9.5 users can now control the number of incoming connections allowed to their deployments. You'll find the new control in the Compose web console for your PostgreSQL database under Settings.\n",
      "----\n",
      "From the big crop of books about Apache Spark, we take a look at the best introductions for beginners, ML deep-dives, and guides for optimizing at scale.\n",
      "----\n",
      "New artificial intelligence systems are using adversarial networks to develop creativity and originality by more fluidly mixing and matching real-world information.\n",
      "----\n",
      "Don Omondi, Campus Discounts' founder and CTO, discusses securing applications with OAuth and shows you how to securely store authentication data using MySQL and MongoDB.\n",
      "----\n",
      "Learn how to query JSON in Cloudant/CouchDB for elements inside an array value.\n",
      "----\n",
      "This screencast introduces Ecto 2.0. It starts with an overview of the entire stack Ecto works with from Erlang to Elixir to Phoenix. Then it walks thru using Ecto with PostgreSQL by creating a mix project, configuring database access, seeding some data, and ending with a simple query.\n",
      "----\n",
      "This page includes a number of utilities to migrate and transform your data. Note that in most cases, tools that are compatible with Apache CouchDB will also work with Cloudant so are included for your reference.\n",
      "----\n",
      "This data visualization playbook series can help data scientists and data analyst professionals understand the key to identifying and creating good data visualizations by taking an in-depth look at data through the lens of data science. The goal is to introduce strategies, techniques, and thorough analysis to turn a set of data points into a visualization, and in doing so, reveal the data science that is often working behind the scenes.\n",
      "----\n",
      "Take a look at some practical applications for specific Spark machine-learning algorithms in three advanced analytics use cases.\n",
      "----\n",
      "I love the feeling of having a new way to think about the world. I especially love when theres some vague idea that gets formalized into a concrete concept. Information theory is a prime example of this.\n",
      "----\n",
      "Machine learning algorithms can be divided into 3 broad categoriessupervised learning, unsupervised learning, and reinforcement learning.\n",
      "----\n",
      "Along with the currently rapid growing interest in AI, there is a rapidly growing tension. On the one hand, top research labs in AI are becoming more openpublishing results to ArXiv and pushing code to github. On the other, competition is becoming more fierce to become the dominant player. From deep learning frameworks, to cloud computing platforms, to customized hardware, the battle is on for who will become the standard for the near (and possibly long term) future of AI technologies.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "With several new datasets uploaded to Datasets this month, we saw a great number of exceptional scripts created. In this months blog featuring the May 2016 Scripts of the Week, youll \n",
      "----\n",
      "This blog post helps you understand the relationship between different dimensions, Python lists, and Numpy arrays as well as showing you some helpful tips.\n",
      "----\n",
      "A few weeks ago Raj Singh and I demonstrated a chatbot called Cognitive Event Finder at IBMs installation at SXSW. The chatbot allowed users to search for tech sessions, music gigs, or film\n",
      "----\n",
      "This helper library for Jupyter notebooks improves the user experience of working with data and makes charting a breeze.\n",
      "----\n",
      "Learn how to deploy a Python application in the IBM Streaming Analytics service on IBM Bluemix, without installing IBM Streams.\n",
      "----\n",
      "bcrypt is an intentionally slow hashing function. While this slowness sounds paradoxical when it comes to password hashing it isn't because both the good guys and the bad are slowed down.\n",
      "----\n",
      "Flows offer an interactive, graphical environment to ingest and clean data, to train and tune models, and to organize your workflow in a visual, collaborative ecosystem of tools. Take advantage of either Spark or SPSS Modeler runtimes to accelerate model development and deployment.\n",
      "----\n",
      "walks through the creation of a sample application called GeoPix, which leverages IBM MobileFirst on IBM Bluemix to capture data and image attachments locally (even offline) and replicate those changes to an online data store so that the user experience is never compromised.\n",
      "----\n",
      "Don Omondi, Campus Discounts' founder and CTO, talks about their use case of fetching data from multiple databases and joining them using RabbitMQ RPCs.\n",
      "----\n",
      "In this article, I want to attempt to highlight a conceptual piece, bias and variance in RL, and attempt to demystify it to some extent. My hope is that in doing so a greater number of people will be able to debug their agents learning process with greater confidence.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "PouchDB is a database. Its a JSON document store to be precise, allowing you to create, read, update, delete and query your documents with a simple JavaScript API. PouchDB is most commonly used in a\n",
      "----\n",
      "This post shows how your suite of microservices can adhere to the philosophy of standalone microservices while you extend/expand them in a scalable & predictable way using service discovery tools.\n",
      "----\n",
      "Lucero Del Alba talks about using PostgreSQL's materialized views to process large amounts of data without slowing down your database.\n",
      "----\n",
      "We hear a lot about who data scientists are. But how should one get better?\n",
      "----\n",
      "Simple Search Service is an example app that easily exposes a faceted search API via Cloudant. Here, we build a UI for searching Game of Thrones characters.\n",
      "----\n",
      "Bradley Holt presents this webinar where you can learn how to deploy a full stack Node.js application to IBM Bluemix and how to quickly and easily add a Cloudant service to your Bluemix application. The value of Cloudant, a NoSQL database as a service (DBaaS) that excels at data replication and sync, is that it makes data available to users non-stop with elastic scalability, high availability, and integrated security. Join Bradley Holt, Developer Advocate for Cloudant, as he explores the Bluemix application manifest, buildpacks, deploy scripts, the Procfile, and configuration of Bluemix applications.\n",
      "----\n",
      "Our colleagues at Cloud Data Services have been blogging a journey through the databases available on IBM Bluemix and the cloud and it's interesting to see how, in seven stops, how many times the destination has been Compose.\n",
      "----\n",
      "It is always amazing when someone is able to take a very hard, present day problem, and translate it to one that has been studied for centuries. This is the case with Word2Vec, which transforms words\n",
      "----\n",
      "We recently sat down with CTO of ScyllaDB, Avi Kivity, to talk about Scylla, its roots, how it approaches being Cassandra compatible, and where Scylla is going from here.\n",
      "----\n",
      "httr 1.2.0 is now available on CRAN. The httr package makes it easy to talk to web APIs from R. Learn more in the quick start vignette. Install the latest version with: install.packages(httr\n",
      "----\n",
      "In this article, we focus on the network address data type and show you how it works, when to use it, and how it can help you when storing IP addresses in your database.\n",
      "----\n",
      "GeoFile is a series dedicated to looking at geographical data, its features and its uses. In this article, we discuss spatial reference systems, what EPSG codes are, and how databases handle and transform them.\n",
      "----\n",
      "When projections showed Hurricane Harvey could bring a record setting amount of rain to Houston, the graphics desk at the New York Times started exploring ways of showing the rainfall. \n",
      "----\n",
      "See what you can do with Cloudant Geospatial and watch a quick intro to using GeoJSON.\n",
      "----\n",
      "Watch this video to see how to use logistic regression classifiers with publicly available data about metabolic diseases to determine if someone has chronic ...\n",
      "----\n",
      "NoSQL doesn't force you to define the schema of your documents up front. That isn't to say that you have no schema in your database, only that you have flexibility to alter it at will. This blog post describes some techniques for how to organise your data to minimise storage while retaining document readability.\n",
      "----\n",
      "This post outlines the data-security aspects one must consider before making non-public relational data available via Jupyter data science notebooks.\n",
      "----\n",
      "Telekinesis, or the ability to move an object with your thoughts, is no longer a far-fetched reality. Using the Muse, a wearable device that collects brain wave signals, an Arduino device, and a Python program, this article demonstrates how you can move a toy car using the power of your mind. This tutorial also describes how a Node.js web application running on IBM Bluemix was developed to store and show the brain activity collected using the Muse.\n",
      "----\n",
      "In part 2, we cover mappings in Elasticsearch\n",
      "----\n",
      "Learn how to build mobile web apps using PouchDB, AngularJS, Node.js and IBM Cloudant, a globally distributed data layer for web and mobile apps. Quickly integrate with Cloudant using the Cloudant Node.js client library and ease the management and deployment of your application with IBM Bluemix.\n",
      "----\n",
      "You might quickly find yourself working with notebooks or RStudio in IBM Data Science Experience (DSX) and get to a point where you would like to access some databases that reside behind some\n",
      "----\n",
      "Must-have skills? Daily challenges? Find out what actual data scientists  really think about their critical role in data science \n",
      "----\n",
      "Are Your Predictive Models like Broken Clocks?\n",
      "----\n",
      "Launch of SETI@IBMCloud: SETI data analysis w/ IBM Bluemix, Data Science Experience, Spark & Object Storage. Built w/ IBM dashDB, Cloudant & Cloud Foundry.\n",
      "----\n",
      "In this post, Ill demo variational auto-encoders [Kingma et al. 2014] on the Frey faces dataset, using the keras deep-learning Python library.\n",
      "----\n",
      "Machine learning explained in 10 minutes.\n",
      "----\n",
      "This video shows you how to upload CSV data for analytics using RStudio in IBM Data Science Experience (DSX).\n",
      "----\n",
      "Is your data tidy or messy? If you are not sure about how to answer this question, don't worry, you'll understand it in a minute. This question has to do with an issue that keeps busy data scientists (or statisticians, or machine learners, pick your favorite). The issue is that of data preparation. It is well known that 80% of the time spent in a data science project is spent in data preparation, and as little as 20% is spent in actually learning from it (or modeling it). What is this data preparation about? Well known steps include dealing with missing values. Should they be replaced by 0, or...\n",
      "----\n",
      "nan\n",
      "----\n",
      "Were spending a lot of time interviewing data scientists around the world to understand how they work and interact with their peers and other stakeholders. What have we found? Today, data science is\n",
      "----\n",
      "To take Microservices into production, you need to make sure they are communicating securely and reliably. We explore using RabbitMQ as an alternative transport for SenacaJS microservices and show you how easy it can be to plug Compose RabbitMQ into your microservices stack.\n",
      "----\n",
      "Python if statements are very commonly used to handle conditions. If you learn data + coding, here's an article to learn the concept and the syntax!\n",
      "----\n",
      "Evolving from MongoDB and Redis to Elasticsearch, Campus Discounts' founder and CTO Don Omondi talks about how and why the company made the switch to power their user recommendation feeds.\n",
      "----\n",
      "Having trouble getting Java to connect to Compose's MongoDB with SSL enabled? Try this one library trick to make everything work from the environment.\n",
      "----\n",
      "Need to convert your Cloudant JSON to a different format? Read how to get JSON data into a spreadsheet, into your calendar, or aggregate data in an RSS reader.\n",
      "----\n",
      "To help you hit the ground running, weve provided some sample scripts to get you started fast with BigInsights. These scripts are tested on BigInsights on Cloud (Bluemix) but should also work for BigInsights on-premises version.\n",
      "----\n",
      "Increase app response time and spare your data service. Use cachemachine and Cloudant's Node.js library to cache any HTTP service with Redis as the cache store.\n",
      "----\n",
      "In this article, we'll connect our application to a MongoDB database and lay the groundwork for a fully-authenticated application with authorized routes.\n",
      "----\n",
      "In this article, we'll be looking at PostGraphQL, a library that provides us with \"a GraphQL schema created by selection over a PostgreSQL schema\". It gives us an easy way connect to a PostgreSQL database, automatically detecting primary keys, relationships, tables, types, and functions, that can then be queried using a GraphQL server. \n",
      "----\n",
      "Use Weather Company Data APIs and matplotlib to generate your own weather forecast map within a python notebook.\n",
      "----\n",
      "In this second in our series on building smart business chatbots, well learn how to give our Slackbot from Part 1 some intelligence by connecting it to a Watson Conversation dialog.\n",
      "----\n",
      "This video provides an overview of the governance features in IBM Data Catalog. \n",
      "----\n",
      "We're going to look connecting your applications to Compose for MySQL databases.\n",
      "----\n",
      "Here we provide a brief overview of some popular database management tools that you can use right now with your Compose for MySQL deployment: MySQL Workbench, Sequel Pro, DBeaver.\n",
      "----\n",
      "A look at Empirical Bayes methods for multiple sample sizes.\n",
      "----\n",
      "Matplotlib is a valuable but misunderstood foundation of the python data science stack.\n",
      "----\n",
      "RabbitMQ is a powerful, flexible message broker that is a great fit for many modern apps. This article provides queue setup strategies that process message failures gracefully.\n",
      "----\n",
      "With rapid changes in data science, machine learning and artificial intelligence, podcasts are a great way to keep yourself updated with new developments.\n",
      "----\n",
      "The dygraphs package is an R interface to the dygraphs JavaScript charting library. It provides rich facilities for charting time-series data in R, including: Automatically plots xts time-series ob\n",
      "----\n",
      "The latest enhancement to Compose makes your life simpler and more secure with the introduction of TLS/SSL certificates backed and verified by Let's Encrypt. We've initially rolled out this new certificate scheme for users of RabbitMQ and Elasticsearch. \n",
      "----\n",
      "Watch how to use IBM dashDB with R to do analytics.\n",
      "----\n",
      "This article explains the concept of recommendation systems in python and builds one using graphlab library. Explains the types of engines too.\n",
      "----\n",
      "With the recent arrival of Let's Encrypt TLS/SSL certificate support on Compose's Elasticsearch and RabbitMQ, we've noticed some issues popping up from the rabbit hole that is this encryption support. \n",
      "----\n",
      "We've taken the beta label off the messaging platform and upgraded our deployed RabbitMQ to version 3.6.5, which includes new features like \"Lazy Queues\".\n",
      "----\n",
      "A survey of command-line tools you can use to interface with IBM Cloudant or Apache CouchDB.\n",
      "----\n",
      "When you need to turn your Mongo database into a RESTFul API, RESTHeart can get you up-and-running quickly. In this article, we'll explore using RESTHeart to expose a RESTFUL API directly from a Mongo database on Compose.\n",
      "----\n",
      "IBM Watson Data Platform offers the only collaborative ecosystem of analytics tools, cognitive services, and data discovery, management, and governance capabilities designed for teams to minimize the time it takes to solve problems.\n",
      "----\n",
      "We discuss some of the issues in computing area under the curve (AUC).\n",
      "----\n",
      "Why Even Try, Man? Irecently came upon Brian Granger and Jake VanderPlass Altair, a promising young visualization library. Altair seems well-suited to addressing Pythons ggplot envy,\n",
      "----\n",
      "If you have data in an Amazon DynamoDB service and want to move it to IBM Cloudant or Apache CouchDB, how would you go about it? First of all, DynamoDB has a peculiar form of JSON. A single\n",
      "----\n",
      "Enhancements to our Simple Search Service app and API.\n",
      "----\n",
      "One-click deployment using IBM Bluemix to host a Wordpress or Ghost environment and Compose for MySQL to maintain the database. Try it if you want more customisation than your typical hosting :D\n",
      "----\n",
      "IBM Cloudant is a fully-managed NoSQL database-as-a-service based on Apache CouchDB. While Cloudants web UI provides a JSON view and a table view of your data, it does not, however, include a\n",
      "----\n",
      "In my last blog Business differentiation through Machine Learning I introduced and described the concepts of machine learning. We traced its origins from a computer science project to Watson show\n",
      "----\n",
      "Use Spark Streaming and IBM Watson to track how a conversation is trending on Twitter\n",
      "----\n",
      "Here, we'll take you from provisioning your own MySQL database on Compose, to connecting to it within a few minutes.\n",
      "----\n",
      "User behavior tracking and analysis for your web app or site. Capture detailed events and actions--even those that don't change the URL.\n",
      "----\n",
      "When your business is all about providing the highest quality experiences for your clients, the last thing you want to worry about is your databases. At Human Design, they have solved that problem by using Compose's MongoDB for all the full stack solutions they create for their non-profit clients\n",
      "----\n",
      "Today we take another step toward making Machine Learning simple and accessible for everyone with our launch of IBM Watson Machine Learning. Watson Machine Learning is designed to make AI and Machine\n",
      "----\n",
      "This video shows you how to add collaborators to projects in IBM Data Science Experience (DSX).\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "We spoke with Christopher Quinones, Project Lead and Developer on Compose for JanusGraph, to get the scoop on graph databases, the JanusGraph community, and the team's usage of Scylla.\n",
      "----\n",
      "Learn how to use the Apache Spark Machine Learning Library (MLlib) in IBM Analytics for Apache Spark on IBM Bluemix. \n",
      "----\n",
      "If we commit the above code to GitHub we divulge our secret API key allowing someone to use our account. This isnt a rare event  many developers accidentally commit their credentials and others\n",
      "----\n",
      "The most common reason why we use maps is to calculate the distance from one point to another. Redis and PostGIS come with commands, GEODIST and ST_Distance, that allow us do this easily, but there are reasons why choosing one database over the other may be a better choice for your use case.\n",
      "----\n",
      "This post is part of a series covering the exercises from Andrew Ng's machine learning class on Coursera.\n",
      "----\n",
      "Generative adversarial networks (GANs) are a class of neural networks that are used in unsupervised machine learning. They help to solve such tasks as image generation from descriptions, getting high\n",
      "----\n",
      "Run your Alexa skill serverless, paying for only for traffic you generate when your skill gets popular.\n",
      "----\n",
      "A report from 2 important conferences covering the latest innovations in Apache Spark, Python, data analytics, and machine learning.\n",
      "----\n",
      "I am excited to moderate a panel discussion at SXSW this year on Offline First. For the uninitiated, in the smallest nutshell, Offline First concerns how and why to build applications that meet\n",
      "----\n",
      "IBM Cloud Data services proudly sponsored the 2016 Girl Develop It Leadership Summit. One of our developer advocates reports back from the event in Austin.\n",
      "----\n",
      "Database-backed microservices are powerful and in this article we show how to use SenecaJS, NodeJS and Compose databases to create a virtual product catalog using them.\n",
      "----\n",
      "When importing or restoring your database into MongoDB, make sure that you don't use the admin database. Why? We'll show you.\n",
      "----\n",
      "IBM dashDB allows ad-hoc querying of data from Cloudant using R or SQL for data warehousing and predictive analytics.\n",
      "----\n",
      "Data Science Experience has helped me to move all of my work that is in Jupyter notebooks and RStudio from my local environment to the cloud. This works well for me because I can now pick up projects\n",
      "----\n",
      "Sep. 2016 marks version 1.5 of the Cloudant Node.js Library. The library comes with a new plug-in system, and Keep-Alive is now on by default.\n",
      "----\n",
      "Node-RED is a tool for wiring together hardware devices, APIs and online services in new and interesting ways.\n",
      "----\n",
      "Video demo and code walkthrough of IBM's sample mobile Acme Apparel app.  Spencer Hockeborn quickly shows you how the app works. Then he steps through the setup on Bluemix, integration with Cloudant database, user authentication setup. and confiration of push notifications. \n",
      "----\n",
      "Rather than replicating the minority observations (e.g., defaulters, fraudsters, churners), Synthetic Minority Oversampling (SMOTE) works by creating synthetic observations based upon the existing minority observations (Chawla et al., 2002). \n",
      "----\n",
      "In this article, we'll show you how to secure your RESTHeart API by adding authentication and role-base access control, as well as enabling SSL encryption.\n",
      "----\n",
      "This video shows you how to add assets to a data catalog. \n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data. \n",
      "----\n",
      "The Simple Data Pipe is an app that moves your Salesforce data to dashDB, which is the IBM cloud data warehouse. Once you have your Salesforce data in dashDB, you can do all kinds of analysis on it, with all kinds of tools, such as SQL, R, and Looker.\n",
      "----\n",
      "How to set up an Elasticsearch database on Compose.\n",
      "----\n",
      "This is the first of a three part series focused on an open source tool chain for small data business intelligence. Part 1 explores the history and reasoning behind dimensional modeling via a use case: online video viewing analysis.\n",
      "----\n",
      "Over the past couple of months our team has been working with Steve Trevathan of Make&Model and Gregor Martynus of Neighbourhoodie on a demo mobile web app app called Voice of InterConnect.\n",
      "----\n",
      "Decisions can be fatiguing and prone to bias, not just for individuals but for whole organizations. The bigger the decision and the more complex the inputs, the greater the consequences of failure\n",
      "----\n",
      "Reaction to the Verge interview with Phil Schiller about upcoming changes to Apple's App Store.\n",
      "----\n",
      "How to build SQL Queries in a Scala notebook using IBM Analytics for Apache Spark\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "Joshua Drake who is one of the founders of United States PostgreSQL, the SPI-PostgreSQL Liaison and a Chair for PgConf US, the largest PostgreSQL conference in North America. Here is his talk from the Compose DataLayer conference.\n",
      "----\n",
      "Looking to learn the basics of cloud databases? In this series, we show how to use them on Compose and the IBM cloud. Enter: etcd.\n",
      "----\n",
      "Learn how to configure a dashDB connection in Cognos Business Intelligence and create stunning visualizations. \n",
      "----\n",
      "At Cloudant, we use curl a lot to interact with Cloudant's HTTP API. 'acurl' is a tool that allows you to interact with Cloudant without having to enter the username and password with every request.\n",
      "----\n",
      "See how to find Cloudant documentation and locate support resources.\n",
      "----\n",
      "Adapting the simple SQL support in my silverlining npm module to work with in-browser database PouchDB\n",
      "----\n",
      "Statistics is difficult. Of course it is, as mostly thats the actual science part in data science. But it doesnt mean that you couldnt learn it by yourself if you are smart and determined enough\n",
      "----\n",
      "Learn how to use a Python notebook for easy access to filter and refine Cloudant data in IBM Analytics for Apache Spark.\n",
      "----\n",
      "If you keep your MongoDB indexes minimal and simple, but then also feed all your records into Elasticsearch as a secondary database, what you get is a database tuned for efficient transactions and a secondary database tunable for speedy, complex and comprehensive searches.\n",
      "----\n",
      "For Spark 1.6, Ive been working to add Pearson correlation aggregation functionality to SparkSQL. The aggregation function is one of the expressions in SparkSQL. It can be used with t\n",
      "----\n",
      "Learn about NoSQL products at IBM! A recap of my talks on \"NoSQL\", including details on infrastructure, data models, and the types of databases available today on the IBM cloud.\n",
      "----\n",
      "Gigi Sayfan takes a dive into database migrations using Compose PostgreSQL as the target database, Python 3.5.1 as the programming language, SQLAlchemy 1.0.13 as the DB access library and Alembic 0.8.6 as the the database migration toolkit. \n",
      "----\n",
      "Are you smarter than Apache Spark? Play our Rock-Paper-Scissors game, powered by IBM Analytics for Apache Spark, then read how we built it.\n",
      "----\n",
      "Chrome extensions are really small web apps, making them great for PouchDB JSON replication & sync. We provide code for two such offline-first apps.\n",
      "----\n",
      "Imagine you have a small business, like a restaurant or a hotel. What are the most common enquiries that you get? Many companies are now creating chat bots to answer questions automatically.\n",
      "----\n",
      "Learn how to incorporate a job queue by connecting your PHP code to RabbitMQ. Example Vagrant environment included.\n",
      "----\n",
      "Custom classifiers can be highly powerful but require careful training and content considerations to be properly optimized. Through our user conversations, weve assembled a best practices guide below to help you get the most out of your custom classifiers.\n",
      "----\n",
      "Learn how to make moving average calculations more high-level and user-friendly using PostgreSQL on Compose.\n",
      "----\n",
      "Just a month after announcing our partnership with Continuum Analytics, we are proud to introduce the integration of Anaconda with Jupyter Notebooks in IBM Data Science Experience. What does that\n",
      "----\n",
      "Store data locally with PouchDB to keep unconnected users working and achieve an offline-first app.\n",
      "----\n",
      "R Markdown marries together three pieces of software: markdown, knitr, and pandoc. This five page guide lists each of the options from markdown, knitr, and pandoc that you can use to customize your R Markdown documents.\n",
      "----\n",
      "A journey to help developers find interesting insights using replay analytics and gamers to ramp up their Starcraft skills.\n",
      "----\n",
      "Webhooks are an excellent way of moving data between applications, but if added without consideration for scaling they can easily become a performance problem.\n",
      "----\n",
      "MapReduce is a two-phase paradigm for crunching large data sets in a distributed system. This video uses a basic example to explain how MapReduce works. \n",
      "----\n",
      "Companies too often overlook data lake concerns like findability, classification and governance, focusing instead on data capture, storage and processing.\n",
      "----\n",
      "Time series analysis using max and min to describe brain signal data (event related potential).\n",
      "----\n",
      "If you're new to MongoDB, this webinar recording is a good first stop for learning more.\n",
      "----\n",
      "Heres this weeks news in Data Science and Big Data.\n",
      "----\n",
      "Look at traffic data from the city, create robust data visualizations that allow users to encapsulate business logic, create charts and graphs, and more.\n",
      "----\n",
      "This is the second of a three part series focused on an open source tool chain for small data business intelligence. Part 1 explored dimensional modeling from a logical and use case perspective. Part 2 focuses on Extract, Transform and Load with NodeJS.\n",
      "----\n",
      "JSON-LD (JSON Linked Data) is a simple way of providing semantic meaning for the terms and values in a JSON document. Providing that meaning with the JSON means that the next developer's application can parse and understand the JSON you gave them.\n",
      "----\n",
      "In this tutorial we will have a look at how you can write a basic for loop in R. It is aimed at beginners, and if youre not yet familiar\n",
      "----\n",
      "Our Node.js library for IBM Graph makes it easy to store and query data using Graph's HTTP API.\n",
      "----\n",
      "The custom model within Watson Visual Recognition is one of the API services most popular functionalities, allowing users to train Watson to recognize virtually any custom content. \n",
      "----\n",
      "Overfitting in machine learning can single-handedly ruin your models. This guide covers what overfitting is, how to detect it, and how to prevent it.\n",
      "----\n",
      "Learn you how to analyze precipitation data using a Python Notebook in IBM Analytics for Apache Spark.\n",
      "----\n",
      "See how to populate data into a table in your IBM dashDB database from a local file residing on your machine. \n",
      "----\n",
      "In the first \"Addon\" article of this cycle of Compose's Write Stuff, Lucero Del Alba takes a look at how to get better performance with PostgreSQL, as long as you aren't too worried about replication and persistence.\n",
      "----\n",
      "Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley or New York? Learn more about the Insight Artificial Intelligence Fellows Program.\n",
      "----\n",
      "Its time to continue our discourse about Statistical Bias Types. What can go wrong during the analysis and the presentation part?\n",
      "----\n",
      "Do you want to leverage Compose, Twilio, and IBM Watson to provide customers with a real-time, interactive experience? We'll show you how, on IBM Bluemix.\n",
      "----\n",
      "Listening to Data Science podcasts is a great way to stay up to date on the latest advancements in the field, coming right from the leading voices of the industry. Here are 10 of our favorite picks.\n",
      "----\n",
      "TL;DR: We use proxies to enable SSL, High Availability and Whitelisting. Because we care.\n",
      "----\n",
      "Probabilistic graphical models tutorial to understand the framework and its applying to machine learning problems.\n",
      "----\n",
      "Every two weeks, we find the most interesting data science links from around the web and collect them in Data Science Briefings, the DataMiningApps newsletter.\n",
      "----\n",
      "How to create a blog entry using Brunel in a few minutes. The data comes from The Guardian and is completely unmodified.\n",
      "----\n",
      "We'll show you how by building an IBM Bluemix application that lets Watson interact with users via Twilio's SMS service then sends their data over to Compose PostgreSQL for storage.\n",
      "----\n",
      "Even in areas and domains where deep learning excels, simpler approaches are worth examining. A while back I noted that there are several considerations when evaluating machine learning models: Acc\n",
      "----\n",
      "Learn about the Apache NiFi data routing and transformation server, with its flow-based programming engine.\n",
      "----\n",
      "Watch how to apply association rules using R to data stored in dashDB using a retail market basket scenario. \n",
      "----\n",
      "Long Short-Term Memory neural networks (LSTMs) are a type of recurrent neural networks that has been used for many state-of-the-art applications in recent years, such as speech recognition, machine translation, handwriting recognition and, of course, time series prediction.\n",
      "----\n",
      "Loren Sands-Ramshaw, author of GraphQL: The New REST shows how to combine data from multiple data sources using GraphQL in part two of this Write Stuff series.\n",
      "----\n",
      "Video tutorial showing the complete creation of an offline-first native iOS app with a Cloudant backend\n",
      "----\n",
      "When we noticed an article about how to connect Compose for MySQL on Bluemix with PHP, we just had to make sure more people saw how simple it was. So over to Lorna Jane, who's allowed us to cross post her blog post.\n",
      "----\n",
      "If youre ready to build interactive web apps with R, say hello to Shiny. This cheat sheet provides a tour of the Shiny package and explains how to build and customize an interactive app.\n",
      "----\n",
      "80 percent of data scientists' time is spent finding, cleansing and organizing data, which leaves only 20 percent to actually perform analysis.\n",
      "----\n",
      "How to use the Spark machine learning programming model in IBM Analytics for Apache Spark on IBM Bluemix\n",
      "----\n",
      "Integrated Mapbox visualizations in Cloudant, a GeoJSON database that lets you build spatial queries interactively and instantly see the results.\n",
      "----\n",
      "Looking to learn the basics of cloud databases? In this series, we show how to use them on Compose and the IBM cloud. Enter: IBM Graph.\n",
      "----\n",
      "This guides objective is to explain the relationship between convolutional layers and transposed convolutional layers and to provide an intuitive understanding of the relationship between input shape, kernel shape, zero padding, strides and output shape in convolutional, pooling and transposed convolutional layers.\n",
      "----\n",
      "Example app showing how to use Cloudant/Apache CouchDB to power analytics with D3 and JSON. \n",
      "----\n",
      "Language models assign probability values to sequences of words. Those three words that appear right above your keyboard on your phone that try to predict the next word youll type are one of the uses of language modeling. In the case shown below, the language model is predicting that from, on and it have a high probability of being the next word in the given sentence. Internally, for each word in its vocabulary, the language model computes the probability that it will be the next word, but the user only gets to see the top three most probable words.\n",
      "----\n",
      "Im pleased to announce tidyr 0.4.0. tidyr makes it easy to tidy your data, storing it in a consistent form so that its easy to manipulate, visualise and model. Tidy data has a simple convention\n",
      "----\n",
      "Build complex real-time features for your app or website, at scale, with the use of a few simple APIs provided by our Simple Notification Service.\n",
      "----\n",
      "PouchDB is in-browser database that syncs with any Couch-like database such as Cloudant. It allows offline-first mobile and web-apps to be created that sync to the cloud when online.\n",
      "----\n",
      "Learn how to use the open crime data we've sourced from local police departments -- run geospatial queries and use our APIs in your web & analytics apps\n",
      "----\n",
      "There are a lot of publications about the concept of boosting. In 1988, Michael Kearns published Thoughts on Hypothesis Boosting, which is probably the oldest one. About the algorithms, it is possible to find some references. Consider for instance Improving Regressors using Boosting Techniques, by Harris Drucker. Or The Boosting Approach to Machine Learning An Overview by Robert Schapire, among many others. In order to illustrate the use of boosting in the context of regression (and not classification, since I believe it provides a better visualisation) consider the section in Dong-Sheng Caos In The boosting: A new idea of building models.\n",
      "----\n",
      "The search index lets you create flexible queries on one or more fields in Cloudant documents. This video shows you how to query a search index.\n",
      "----\n",
      "Random Forest is a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, a great result most of the time. In this post, you are going to learn, how the random forest algorithm works and several other important things about it.\n",
      "----\n",
      "A very flexible tool to create readable analyses: one can keep code, images, comments, formula and plot together.\n",
      "----\n",
      "How to build SQL Queries in a Scala notebook using IBM Analytics for Apache Spark\n",
      "----\n",
      "In this first in a series of futuristic interaction articles, well use Bluemix+Compose to build a chatbot that can reliably converse with our users while collecting data and gaining insights on their personality and mood.\n",
      "----\n",
      "Cloudant Search's Lucene index allows simple geographic searches to be performed by querying within latitude/longitude ranges. This post shows you how.\n",
      "----\n",
      "Working with Data Science Experience comes with a flexible storage option of IBM Cloud Object Storage. \n",
      "----\n",
      "In the multibillion dollar world of sports entertainment, we often think of injuries as being chance events. I set out to see whether the statistical richness of baseball could be mined to identify players at risk of injury. \n",
      "----\n",
      "Cloudant Search, based on Apache Lucene, allows full-text and field search across your JSON document store. This post demonstrates the feature set using data from lobbyist disclosures from the US Senate.\n",
      "----\n",
      "A dive into etcd and the creation of a Python library to manage dynamic configuration.\n",
      "----\n",
      "The Unix diff command-line utility has been around since the 1970s. It compares two text files line-by-line and tells you the differences between them. If I diff two files a.txt and b.txt containing\n",
      "----\n",
      "The Simple Data Pipe is an example app for moving data into dashDB, IBMs cloud data warehouse. Here, we land Stripe.com JSON in dashDB.\n",
      "----\n",
      "nan\n",
      "----\n",
      "IBM Cloudant is a NoSQL JSON document store thats optimized for handling heavy workloads of concurrent reads and writes in the cloud; a workload that is typical of large, fast-growing web and mobile apps. You can use Cloudant as a fully-managed DBaaS running on public cloud platforms like IBM SoftLayer or via an on-premise version called Cloudant Local, that you can run yourself on any private, public, or hybrid cloud platform you choose.\n",
      "----\n",
      "Enriching and analyzing podcast metadata using Jupyter Notebooks.\n",
      "----\n",
      "This cheat sheet is meant to be a quick reference to our feature set, so you can know in just a matter of seconds what our services can do.\n",
      "----\n",
      "In this article, we build upon our last discussion of getting the distance between two points, and now move to getting all data points within a specified radius.\n",
      "----\n",
      "At Compose, we use Leftronic for metrics at-a-glance. Leftronic is a real-time, browser-based data visualization service. We like it so much that we wanted to share how we're using it because we think you might like it, too.\n",
      "----\n",
      "Compose customer, Interloop, has launched a \"sales execution and forecasting platform\"  powered by IBM Watson and Compose-hosted MongoDB  to help businesses understand where to focus to close more business predictably.\n",
      "----\n",
      "Each of these new options brings new capabilities to the application stacks that developers can deploy.\n",
      "----\n",
      "Antonio Chavez's talk from the 2017 DataLayer conference, discussing why Queue Technologies moved away from MongoDB.\n",
      "----\n",
      "This video shows you how to use the Spark SQL programming model within a notebook in IBM Data Science Experience (DSX).\n",
      "----\n",
      "The largely dominant meritocratic paradigm of highly competitive Western cultures is rooted on the belief that success is due mainly, if not exclusively, to personal qualities such as talent, intelligence, skills, efforts or risk taking. Sometimes, we are willing to admit that a certain degree of luck could also play a role in achieving significant material success. But, as a matter of fact, it is rather common to underestimate the importance of external forces in individual successful stories. \n",
      "----\n",
      "A simple guide to creating, reading, updating and deleting documents using Cloudant's simple HTTP API.\n",
      "----\n",
      "This article is an overview of the most popular anomaly detection algorithms for time series and their pros and cons.\n",
      "----\n",
      "Announcing tidyr 0.5.0. tidyr makes it easy to tidy your data, storing it in a consistent form so that its easy to manipulate, visualise and model. Tidy data has a simple convention\n",
      "----\n",
      "With RStudio in DSX, git comes pre-installed in your container and RStudio by default have git and subversion integration available.\n",
      "----\n",
      "Taking my Alexa demo to a Jeff conference\n",
      "----\n",
      "Using pseudo-labeling a simple semi-supervised learning method to train machine learning models with sci-kit learn and Python (examples with code).\n",
      "----\n",
      "The idea presented there is  to break the code into four files, all stored in your project directory.\n",
      "----\n",
      "Learn how to use a Python notebook for easy access to filter and refine Cloudant data in IBM Analytics for Apache Spark using the Cloudant-spark connector.\n",
      "----\n",
      "The Sparkling.data library is a tool to simplify and enable quick data preparation prior to any analysis step in Spark. The library provides tools to map, visualize, and transform data for iterative\n",
      "----\n",
      "When it comes to the business value of data, consider another way to look at datawhether it is repetitive data or non-repetitive data.\n",
      "----\n",
      "APIs make our lives easier. As developers, we are all consumers of APIs built and maintained by service providers. Its important to remember that this relationship is a two-way street. To ensure a\n",
      "----\n",
      "Let's look at the problem of viewing queries sent to a server, and how to solve it by using Psycopg's mogrify.\n",
      "----\n",
      "This is part 1 of a series of tutorials, in which we develop the mathematical and algorithmic underpinnings of deep neural networks from scratch and implement our own neural network library in Python, mimicing the TensorFlow API.\n",
      "----\n",
      "The R package DT v0.2 is on CRAN now. You may install it from CRAN via install.packages(DT) or update your R packages if you have already installed it before. It has been over a year \n",
      "----\n",
      "Wes McKinney, Software Engineer, Cloudera Hadley Wickham, Chief Scientist, RStudio This past January, we (Hadley and Wes) met and discussed some of the systems challenges facing the Python and R op\n",
      "----\n",
      "Sean looks back on his first encounter with Spark, talks about building community and staying motivated, and dives deep on Spark SQL.\n",
      "----\n",
      "Enrich unstructured data from Facebook using a Jupyter Notebook with Watson Visual Recognition, Natural Language Understanding, and Tone Analyzer, then use PixieDust to explore the results and uncover hidden insights.\n",
      "----\n",
      "Watch how to extract and export dashDB data to a CSV file for import into a spreadsheet application such as Microsoft Excel or Open Office. \n",
      "----\n",
      "ReadMe founder Gregory Koberger sat down with us to talk about their quest to make developer documentation easy, and how they chose Compose for their data solution to focus on what truly makes ReadMe unique.\n",
      "----\n",
      "If you are a Compose user, then from today, you will find a new Resources console ready for you to take more control of your database scaling. And if you use Redis on Compose you'll find that new console has enabled support for Read-only slaves.\n",
      "----\n",
      "Self-driving cars were just the start. What's the future of big data-driven technology and design? In a thrilling science talk, Kenneth Cukier looks at what's next for machine learning -- and human knowledge.\n",
      "----\n",
      "Here's the deal, you've probably never heard of SystemML, but you definitely need to know what it is. Why? Not only will SystemML make you look awesome because machine learning is the hot topic right now, but it will also save you a lot of time and trouble.\n",
      "----\n",
      "The Apache Spark SQL component has several sub-components including Analyzer, which plays an important role in making sure that the logical plan is fully resolved at the end of an analysis phase. Analyzer takes a parsed logical plan as input and makes sure all the table references, attributes/column references, and function references are resolved by looking up the metadata from catalogs. It works by applying a set of rules on the logical plan  and transforming it on each stage in order to resolve specific portions of the plan.\n",
      "----\n",
      "This talk will explore why developer experience matters, what makes for a great developer experience and the relationship between developer experience and the broader field of user experience. Software developers are gaining more influence over the purchase decisions of technologies with which they must build on and with which they must integrate. For example, the success of Amazon Web Services, Heroku and MongoDB has been driven primarily by individual software developers choosing to use these tools, rather than the by the decisions of managers or business executives.\n",
      "----\n",
      "Shiny 0.12 has been released to CRAN! Compared to version 0.11.1, the major changes are: Interactive plots with base graphics and ggplot2 Switch from RJSONIO to jsonlite For a full list of changes \n",
      "----\n",
      "A neural network that simulates the way moths recognize odors also shows how they learn so much faster than machines.\n",
      "----\n",
      "How connectors let you load data from a variety of cloud-based sources, through the Simple Data Pipe, and into Cloudant. Use and create your own connectors.\n",
      "----\n",
      "Open data repositories are valuable for many reasons, including:(1) they provide a source of insight and transparency into the domains and organizations that\n",
      "----\n",
      "We are excited to announce that a new package leaflet has been released on CRAN. The R package leaflet is an interface tothe JavaScript library Leafletto create interactive web maps. It was devel\n",
      "----\n",
      "How to analyze data in IBM BigInsights on Cloud using Text Analytics\n",
      "----\n",
      "Data scientists spend a lot of time and effort manipulating data to conform to chart tool requirements, especially when they want to change visualizations. Brunel Visualization Language is a high\n",
      "----\n",
      "DSX offers a wealth of functionality to any software developer, especially those interested in data science. An important part of that functionality is the ability to use Notebooks, which are a convenient and intuitive way to compartmentalize different segments of a code base.\n",
      "----\n",
      "This post discusses a very useful statistical method for estimating a large number of proportions - empirical Bayes estimation.\n",
      "----\n",
      "Leveraging Technology, an enterprise integrator, talks how JavaScript tools like D3.js and Node.js are helping the company build new products on IBM Bluemix\n",
      "----\n",
      "Holden Karau presented this important work at Spark Summit East in NYC in February 2016. See the slides on SlideShare Beyond parallelize and collect  Spark Summit East 2016 from Holden Karau\n",
      "----\n",
      "Data visualization with R: How to get and show meaningful metrics for a scrum team\n",
      "----\n",
      "Automatic backup retrieval is now possible with Compose's API and here we'll show you how to use it from Node.js. If you want to archive your own backups, start here.\n",
      "----\n",
      "In June 2016, members of the Offline First community gathered for a retreat at a house nestled in the Catskill Mountains. Here's a wrap-up of our progress.\n",
      "----\n",
      "Survey visualization techniques that data scientists in all industries need to know\n",
      "----\n",
      "Lucero Del Alba takes a look at how to get better performance out of jsonb data types in PostgreSQL in this Compose's Write Stuff article.\n",
      "----\n",
      "How easy it is to build a learning machine?Shouldn't one just hire some Machine Learning PhDs and have them run their algorithms? Well, this is most probably a good idea, but it won't be enough. I'll try to explain why in this blog entry. Before answering our questions, let's define what we are dealing with. A Learning Machine is a machine (a software, a web site, a mobile app, a robot, pick your favorite) that performs a task, and that gets better and better as it performs it. In recent years, some learning machines made headlines. For instance, IBMWatson defeated best humans at...\n",
      "----\n",
      "Shiny 0.13.0 is now available on CRAN! This release has some of the most exciting features weve shipped since the first version of Shiny. Highlights include: Shiny Gadgets HTML templates Shiny mod\n",
      "----\n",
      "Enhancing our Flight Predict notebook with an interactive app and visualizations built using PixieDust, the open source Python helper library.\n",
      "----\n",
      "In this third article in our the series, we'll take a small back step and use the Database Identity Manager to secure our credentials. Then, we'll use the Bluemix and CloudFoundry CLIs to deploy RESTHeart to the cloud using the IBM Bluemix Container Service.\n",
      "----\n",
      "Cloudant allows custom Javascript to be run server-side to generate indexes for MapReduce and Lucene search indexes. This article outlines common pitfalls, their solutions and advocates for automated testing of such code.\n",
      "----\n",
      "This video shows you how to create a project in IBM Data Science Experience (DSX).\n",
      "----\n",
      "[A version of this post appears on the OReilly Radar.] The OReilly Data Show Podcast: Danny Bickson on recommenders, data science, and applications of machine learning. Subscribe to the O&#\n",
      "----\n",
      "Our forty first release of a weekly round up of interesting Data Science and Big Data news, links, and upcoming events.\n",
      "----\n",
      "As a data scientist, I often get the question,What do you actually do? Come read about the actual day-to-day of a data scientist.\n",
      "----\n",
      "Couchimport is a command-line tool that allows CSV files to be imported in bulk into Cloudant. It can also be used to import streaming JSON sources too. This document tells you how.\n",
      "----\n",
      "A handful of talks at the recent Spark Summit in San Francisco neatly underscore three major trends in genomics:      Faster processing of raw genomic data.     New schema and libraries for genomic analysis.     Genomics as a guide to real-world treatment.  All three trends are moving the center of innovation away from the wetware sequencing process into the realm of computation  where Apache Spark is playing an increasingly key role.\n",
      "----\n",
      "At Offline Camp, I proposed a session on identifying how Offline First applications can improve, and even save, peoples lives. We had a great discussion, which Id like to share. To build an\n",
      "----\n",
      "Built on Meteor.js with a Compose MongoDB and RabbitMQ backend, TradeDepot allows product manufacturers to receive orders from distributors and manage the order all the way through to retail outlets.\n",
      "----\n",
      "A command-line tool that lets you share real-time, streaming text data with your colleagues, who can view logs in their terminal or on a web page.\n",
      "----\n",
      "Traditional approaches to string matching such as the Jaro-Winkler or Levenshtein distance measure are too slow for large datasets. Using TF-IDF with N-Grams...\n",
      "----\n",
      "See how to use cURL, jq, JSONView, POSTMan, and RESTCLient to create, read, update, and delete data in a Cloudant database. \n",
      "----\n",
      "See an example of a chat application we put together which uses just Redis and Go. Redis is full of neat things that make writing an application like this so much easier.\n",
      "----\n",
      "If you look at the wikipedia article for the data processing inequality, its really just a stub (as of the time this article was published\n",
      "----\n",
      "Describes a day in the life of Ryan Irwin, a data engineer at Yelp.\n",
      "----\n",
      "This book will teach you how to do data science with R: Youll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, youll learn how to clean data and draw plotsand many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. Youll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. Youll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.\n",
      "----\n",
      "You can use our Simple Notification Service to power almost any realtime feature. Here, we show how to build an online multiplayer game that updates for realtime play.\n",
      "----\n",
      "Learn how Bluemix apps can access existing relational DB2 on-premises databases using the Secure Gateway service.\n",
      "----\n",
      "If you've developed your Redis application on a local single Redis instance, the move to Compose Redis may expose some assumptions about Redis behavior that you've inadvertently baked into your code.\n",
      "----\n",
      "Here is a roundup of the news this week in Data Science and Big Data. \n",
      "----\n",
      "There's new Compose API calls and a new version of Bach, the Compose command line. We've also been busy streamlining the process of raising a support ticket so there's less stress for you.\n",
      "----\n",
      "Using Cloudant and MobileFirst services on Bluemix we show how to build a Cloudant-backed iOS app which uses Google+ for authentication.\n",
      "----\n",
      "The Apache Spark website documents the properties you can configure, including settings that control the Spark application and the Spark SQL Context. Lets look at some of the Spark SQL Context parameters, and how to enable them with a nice feature in Spark SQL 1.6.\n",
      "----\n",
      "In Elasticsearch, query string queries are their own breed of query - loads of functionality for full text search rolled into one sweet little package. In this article, we'll take a closer look at why query string queries are special and how you can make use of them.\n",
      "----\n",
      "How I used MapBox and The Weather Company data to create a regional weather forecast\n",
      "----\n",
      "This article gives you five examples which show you that social network analytics (SNA) can be applied in different and innovative ways not linked to who likes who and who follows who.\n",
      "----\n",
      "One of the best things about Apache Spark is that it makes real-time analytics of vast unstructured datasets  like social media sites  feasible and affordable for companies of all sizes. But what are the practicalities of performing this kind of analysis? And how would you get started? Chetna Warade, Developer Advocate at IBM, is a software engineer who works in research and product development. We spoke to Chetna about a recent project to demonstrate the potential of Spark for social media analytics, which focused on the popular Ask Me Anything (AMA) section of the social news and entertainment site, Reddit. More...\n",
      "----\n",
      "We'll look at raster data and how you can use shapefiles and rasters to give you other perspectives on your maps by superimposing georeferenced images onto your maps.\n",
      "----\n",
      "Try IBM dashDB, the cloud-based data warehouse, by loading some of your data from IBM PureData System for Analytics (formerly Netezza).\n",
      "----\n",
      "Most large-scale commercial and social websites recommend options, such as products or people to connect with, to users. Recommendation engines sort through massive amounts of data to identify potential user preferences. This article, the first in a two-part series, explains the ideas behind recommendation systems and introduces you to the algorithms that power them. In Part 2, learn about some open source recommendation engines you can put to work.\n",
      "----\n",
      "Use IBM Analytics for Apache Spark in IBM Bluemix, the open cloud platform for building, running, and managing applications\n",
      "----\n",
      "A data browser, nodetool/JMX access and a version update to 1.6 are the latest enhancements to ScyllaDB at Compose.\n",
      "----\n",
      "How to use the Electron framework to convert your web app into a native, installed app.\n",
      "----\n",
      "A lot of people think you need a PhD or tons of experience to get a job in deep learning, but if you're already a decent engineer, you can pick up the requisite skills and techniques pretty quickly. At least, that's our philosophy. (So even if you're a beginner with deep learning, you're welcome to apply for one of our open positions.)\n",
      "----\n",
      "How to implement real-time Spark streaming analytics and publish the output to a live dashboard web app featuring dynamic graphics that update continuously.\n",
      "----\n",
      "this article displays the list of machine learning algorithms such as linear, logistic regression, kmeans, decision trees along with Python R code\n",
      "----\n",
      "Bluemix Helper for adding support for Single Sign On service to your application\n",
      "----\n",
      "Like a lot of data scientists (I'm not there yet, but I aspire to become one), I try my best to keep up with the latest discoveries in a ver...\n",
      "----\n",
      "This video shows how to interact with Cloudant's HTTP API.\n",
      "----\n",
      "Cloudant co-founder Mike Miller talks about Cloudant's origins as database used build to deal with huge datasets produced by the Large Hadron Collider project at CERN.\n",
      "----\n",
      "Compose users can now add-on a dedicated Kibana capsule to their Elasticsearch deployments.\n",
      "----\n",
      "Cloudant's Geospatial features allow GIS applications to built for a fraction of a cost of traditional solutions. This blog post describes the choices you face when choosing a Geospatial index for your applicaiton.\n",
      "----\n",
      "How I gave my Alexa Skill a memory with Redis and OpenWhisk\n",
      "----\n",
      "A year ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science masters program using online resources. \n",
      "----\n",
      "The two main new features of Brunel 0.8 are an enhanced UI for building (as described by Dan) and a through re-working of our code for mapping data to color. This post is going to talk about the latter  with a lot of examples!\n",
      "----\n",
      "Let me be very clear: you need to know your essential toolkit inside and out. You need to remember your tools, and you need to be able to execute quickly and on command if you want to be a top performer.\n",
      "----\n",
      "An introduction to Pandas. A quick start guide to familiarize SAS users with Python and Python's various scientific computing tools.\n",
      "----\n",
      "What exact skills do companies look for when they are recruiting a Data Scientist.\n",
      "----\n",
      "Tutorial on migrating JSON data from the Iris Couch service to IBM Cloudant, all using Apache CouchDB-based replication.\n",
      "----\n",
      "we take a look at long-time Compose customer, Emarsys, who runs Compose-hosted MongoDB, PostgreSQL and Redis for their micro-services architected marketing automation platform.\n",
      "----\n",
      "Varun Singh, a software engineer at IBM's Watson Health, looks at how to use MongoDB and Redis to manage biomedical data in this guest article.\n",
      "----\n",
      "This video shows you how to create and administer a data catalog in Watson Data Platform.\n",
      "----\n",
      "With the latest 0.2.1 version of Transporter, we've been making our open sourced tool for moving and manipulating data between databases even easier. In this short series of articles, we are going to show you how to get your data moving with Transporter.\n",
      "----\n",
      "Audio super-resolution aims to reconstruct a high-resolution audio waveform given a lower-resolution waveform as input. There are several potential applications for this type of upsampling in such\n",
      "----\n",
      "Since then, this metric has been ubiquitously quoted because its both accurate and data cleansing is crucial to the success of any analytics project. This post focuses on data tidying, which is a\n",
      "----\n",
      "Build a word game app and see how to manage and deploy on Bluemix. Explore Bluemix DevOps services and see how to store game data with Cloudant.\n",
      "----\n",
      "Use Redis and and Python scripts to speed your geospatial queries.\n",
      "----\n",
      "In this post, youll learn to query, update, and create SQLite databases in Python.  Well also show you how to use the Pandas package to speed up your workflow.\n",
      "----\n",
      "Adron Hall of Thrashing Code and Home Depot, talks about the problems that arise when connecting immutable application infrastructure with data infrastructure.\n",
      "----\n",
      "Describes the use of Laplace noise in machine learning models.\n",
      "----\n",
      "A full guide to Elasticsearch, the real-time distributed search and analytics engine. This book was written by Clinton Gormley and Zachary Tong and is soon to be released by O'Reilly Media.\n",
      "----\n",
      "See how quick and easy it is to set up a dashDB instance in IBM Bluemix and load data to perform analytics in dashDB.\n",
      "----\n",
      "The relational database has been the dominant model for persisting data for the last 40 years. While SQL databases arent going away anytime soon, the NoSQL (\"not only SQL\") movement has challenged the relational database's place as the default persistence layer for modern applications. Learn about horizontal scaling and eventual consistency as well as key-value stores, document databases, graph databases, and more.\n",
      "----\n",
      "Building your first data warehouse doesnt have to be as enterprise-y and scary as it sounds, especially in the era of cloud services. A good first iteration of your warehousing environment is a simple architecture that we BI architects like to call an Operational Data Store (ODS).\n",
      "----\n",
      "In my last blog Business differentiation through Machine Learning I introduced and described the concepts of machine learning. We traced its origins from a computer science project to Watson show\n",
      "----\n",
      "MongoDB's aggregation pipeline makes finding duplicate documents easier by allowing you to customize how documents are grouped together and filtered.\n",
      "----\n",
      "Which write API endpoint is the right write call for you?\n",
      "----\n",
      "Nothing spoils a plot like (too much) data.\n",
      "----\n",
      "Getting started with custom visualizations, simple tables & word clouds.\n",
      "----\n",
      "Although it is built around a JavaScript engine, many people approach the Mongo Shell as simply a way of entering queries, updates and administration commands. But that JavaScript engine opens up a world of possibilities for making a MongoDB user's life easier and more efficient when it comes to herding the data.\n",
      "----\n",
      "Last week I attended the GeoPython conference in Basel, Switzerland, where a group of enthusiastic people shared their work on geodata and Python. Depending on your data you can choose to represent\n",
      "----\n",
      "In this post, we will go through how to read and write data from and to Amazon S3 using Python 2 with Spark 2.0. Scala also has a similar api. You can use the code below in IBM Data Science\n",
      "----\n",
      "As more devices become internet enabled, harnessing that data to provide value for consumers is becoming an essential strategy for many businesses. Some utility companies, for example, offer smart\n",
      "----\n",
      "Continuing my previous work on exploring Arlington's Bikeometer data, I have decided to look at all of Arlington's counters in this article.  My goal is to retrieve all of the counters available via t\n",
      "----\n",
      "Lua is a compact language which can be embedded in other applications -- as diverse as World of Warcraft and the Nginx web server. And Redis, which is why we are here.\n",
      "----\n",
      "PouchDB uses MapReduce as its default search mechanism but that's about to change. Garren Smith gives an insight into how PouchDB's find plugin works and its relationship to the Cloudant Query search technology that uses a MongoDB-style query language.\n",
      "----\n",
      "We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. \n",
      "----\n",
      "In order to demystify some of the magic behind machine learning algorithms, I decided to implement a simple machine learning algorithm from scratch.\n",
      "----\n",
      "Learn how to use IBM dashDB as data store for Apache Spark. See how dashDB lets you analyze data loaded from Spark. \n",
      "----\n",
      "Once you get used to developing in a Notebook environment, it can be painful to go back to traditional IDEs. In traditional IDEs, you execute your entire script and get a single output. This is great\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in df_content['doc_description']:\n",
    "    print(i)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_df(df=df, df_content=df_content):\n",
    "    \"\"\" Function to create a dataframe that contains all articles, titles\n",
    "    INPUT\n",
    "        df - pandas dataframe describe user interaction with articles\n",
    "        df_content - pandas dataframe describe articles on the platform\n",
    "        \n",
    "    OUTPUT\n",
    "        df_whole - pandas dataframe contains all articles in the platform\n",
    "        article_content - pandas dataframe describe the content of each article on the platform\n",
    "    \"\"\"\n",
    "    # Get the dataframe of the full articles\n",
    "    df_one = df[['article_id', 'title']]\n",
    "    df_two = pd.DataFrame({'article_id': df_content_copy['article_id'].values,\n",
    "                          'title': df_content_copy['doc_description']})\n",
    "    df_whole = pd.concat([df_one, df_two], ignore_index = True)\n",
    "    df_whole.drop_duplicates(subset=['article_id'], inplace=True)\n",
    "    df_whole.sort_values(by='article_id', inplace=True)\n",
    "    df_whole.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "    df_vec = pd.DataFrame(vectorizer.fit_transform(df_whole['title'].values).toarray(),\n",
    "                                                   columns=[*vectorizer.vocabulary_])\n",
    "    \n",
    "    # Combine df_whole and df_vec\n",
    "    df_arts = pd.concat([df_whole, df_vec], axis=1)\n",
    "    \n",
    "    return df_whole, df_arts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whole, df_arts = create_content_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>detect malfunctioning iot sensors with streami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                              title\n",
       "0        0.0  detect malfunctioning iot sensors with streami...\n",
       "1          1  See the forest, see the trees. Here lies the c..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_whole.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_df(df=df_arts):\n",
    "    \"\"\" Function to create a dataframe that contain article similarities by using dot product\n",
    "    INPUT\n",
    "        df - a dataframe consisting of article contents\n",
    "    OUTPUT\n",
    "        article_similarity - a dataframe consisting of article similarities\n",
    "    \"\"\"\n",
    "    # subset articles content\n",
    "    article_content = np.array(df_arts.iloc[:, 2:])\n",
    "    \n",
    "    # Take the dot product to get a article * article matrix of similarities\n",
    "    dot_prod_articles = article_content.dot(np.transpose(article_content))\n",
    "    \n",
    "    # Create a dataframe\n",
    "    article_similarity = pd.DataFrame(dot_prod_articles,\n",
    "                                     index=df_arts.article_id,\n",
    "                                     columns=df_arts.article_id)\n",
    "    return article_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_similarity = create_similarity_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1434.0</th>\n",
       "      <th>1435.0</th>\n",
       "      <th>1436.0</th>\n",
       "      <th>1437.0</th>\n",
       "      <th>1439.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1441.0</th>\n",
       "      <th>1442.0</th>\n",
       "      <th>1443.0</th>\n",
       "      <th>1444.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015735</td>\n",
       "      <td>0.007516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  1311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0.0     1.0       2.0       3.0       4.0       5.0     6.0     \\\n",
       "article_id                                                                   \n",
       "0.0            1.0     0.0  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "1.0            0.0     1.0  0.014732  0.006427  0.013259  0.016704     0.0   \n",
       "\n",
       "article_id    7.0       8.0     9.0     ...  1434.0    1435.0  1436.0  1437.0  \\\n",
       "article_id                              ...                                     \n",
       "0.0         0.000000  0.000000     0.0  ...     0.0  0.000000     0.0     0.0   \n",
       "1.0         0.011091  0.015038     0.0  ...     0.0  0.058803     0.0     0.0   \n",
       "\n",
       "article_id  1439.0    1440.0    1441.0  1442.0  1443.0  1444.0  \n",
       "article_id                                                      \n",
       "0.0            0.0  0.000000  0.000000     0.0     0.0     0.0  \n",
       "1.0            0.0  0.015735  0.007516     0.0     0.0     0.0  \n",
       "\n",
       "[2 rows x 1311 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_similarity.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_user_articles(user_id, df=df):\n",
    "    \"\"\"Function to provide a list of the article_ids sorted by interactions number\n",
    "    INPUT\n",
    "        user_id - (int) a user id\n",
    "        user_item - (pandas df) dataframe of user interaction\n",
    "    OUTPUT\n",
    "        article_ids - (list) a sorted list of the article ids seen by the user\n",
    "    \"\"\"\n",
    "    df_user = df[df.user_id == 1]\n",
    "    df_user = df_user.groupby('article_id').count()\n",
    "    df_user.sort_values('user_id', ascending=False, inplace=True)\n",
    "    \n",
    "    count_article = df_user.user_id.unique()\n",
    "    article_ids = []\n",
    "    for k in count_article:\n",
    "        ids = df_user[df_user.user_id == k].index\n",
    "        article_ids.append(list(ids))\n",
    "        \n",
    "    return article_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_recommends(user_id, m=10, df_simly=article_similarity, thd=1):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        user_id - (int) a user id\n",
    "        m - (int) the number of recommendations you want for the user\n",
    "        df_smily - (pandas dataframe) dataframe that contains the articles similarities using dot product\n",
    "    OUTPUT\n",
    "        recs - (list) a list of recommendations for the user by article id\n",
    "        rec_names - (list) a list of recommendations for the user by article title\n",
    "    \"\"\"\n",
    "    \n",
    "    list_ids = top_user_articles(user_id)\n",
    "    recs = []\n",
    "    \n",
    "    for ids in list_ids:\n",
    "        top_articles = article_similarity.loc[ids].sum()\n",
    "        top_articles.sort_values(ascending=False, inplace=True)\n",
    "        top_articles = top_articles[top_articles >= thd]\n",
    "        article_not_recs = np.setdiff1d(np.array(top_articles.index),\n",
    "                                      np.array(recs))\n",
    "        recs.extend(list(article_not_recs))\n",
    "        \n",
    "        # If there are more than m\n",
    "        if len(recs) > m:\n",
    "            break\n",
    "            \n",
    "    recs = recs[:10]\n",
    "    rec_names = get_article_names(recs, df=df_whole)\n",
    "    \n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Heres this weeks news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2   * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "2  Heres this weeks news in Data Science and Bi...   \n",
       "3  Learn how distributed DBs solve the problem of...   \n",
       "4  This video demonstrates the power of IBM DataS...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  \n",
       "2         This Week in Data Science (April 18, 2017)       Live           2  \n",
       "3  DataLayer Conference: Boost the performance of...       Live           3  \n",
       "4      Analyze NY Restaurant data using Spark in DSX       Live           4  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_body           0.013321\n",
       "doc_description    0.002854\n",
       "doc_full_name      0.000000\n",
       "doc_status         0.000000\n",
       "article_id         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.isnull().sum()/len(df_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have put together your content-based recommendation system, use the cell below to write a summary explaining how your content based recommender works.  Do you see any possible improvements that could be made to your function?  Is there anything novel about your content based recommender?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since `doc_full_name` doesn't have any NULL values, I decided to use __doc_full_name__ variable to compute similarities between articles. By using natural language processing (NLP) technics, I got articles similarity matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use your content-recommendation system to make recommendations for the below scenarios based on the comments.  Again no tests are provided here, because there isn't one right answer that could be used to find these content based recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([310.0, 668.0, 1170.0, 1183.0, 1363.0, 1406.0, 1430.0, 1431.0, 20.0, 43.0],\n",
       " ['time series prediction using recurrent neural networks (lstms)',\n",
       "  'shiny: a data scientists best friend',\n",
       "  'apache spark lab, part 1: basic concepts',\n",
       "  'categorize urban density',\n",
       "  'predict loan applicant behavior with tensorflow neural networking',\n",
       "  'uci: iris',\n",
       "  'using pixiedust for fast, flexible, and easier data analysis and experimentation',\n",
       "  'visualize car data with brunel',\n",
       "  'working interactively with rstudio and notebooks in dsx',\n",
       "  'deep learning with tensorflow course by big data university'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a recommendations for a user who only has interacted with article id '1427.0'\n",
    "df_1427 = df[df['article_id']==1427.0].groupby('user_id').count()\n",
    "user_1427 = df_1427.sort_values('article_id').index[0]\n",
    "\n",
    "# make recommendation\n",
    "content_recommends(user_1427)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, you will build use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` You should have already created a **user_item** matrix above in **question 1** of **Part III** above.  This first question here will just require that you run the cells to get things set up for the rest of **Part V** of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick look at the matrix\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` In this situation, you can use Singular Value Decomposition from [numpy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) on the user-item matrix.  Use the cell to perform SVD, and explain why this is different than in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform SVD on the User-Item Matrix Here\n",
    "\n",
    "u, s, vt = # use the built in to get the three matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Use the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below: \n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': # letter here, \n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?': # letter here, \n",
    "    'How many articles can we make predictions for in the test set?': # letter here,\n",
    "    'How many articles in the test set are we not able to make predictions for because of the cold start problem?': # letter here\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now use the **user_item_train** dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "Use the cells below to explore how well SVD works towards making predictions for recommendations on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit SVD on the user_item_train matrix\n",
    "u_train, s_train, vt_train = # fit svd similar to above then use the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use these cells to see how well you can use the training \n",
    "# decomposition to predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make with any of the above recommendation systems are an improvement to how users currently find articles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conclusions'></a>\n",
    "### Extras\n",
    "Using your workbook, you could now save your recommendations for each user, develop a class to make new predictions and update your results, and make a flask app to deploy your results.  These tasks are beyond what is required for this project.  However, from what you learned in the lessons, you certainly capable of taking these tasks on to improve upon your work here!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "> Congratulations!  You have reached the end of the Recommendations with IBM project! \n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the [rubric](https://review.udacity.com/#!/rubrics/2322/view). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
